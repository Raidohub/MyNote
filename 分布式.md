# **分布式**


## 分布式唯一ID

### 数据库主键自增

- 数据库个数取余为步长值

### redis

- 利用redis的单线程，采用原子操作INCR、INCRBY、SETNX实现

### UUID(8-4-4-4-12)

- 简单，但长度很长，容易占用大量存储空间，在建立索引和基于索引查询时都存在性能问题

### Snowflake

- 19位有序Long类型

### Zookeeper

- 通过其znode数据版本来生成序列号，可以生成32位和64位的数据版本号，客户端可以使用这个版本号来作为唯一的序列号

### MongoDB的ObjectId

- 芒果的ObjectId和snowflake算法类似。它设计成轻量型的，不同的机器都能用全局唯一的同种方法方便地生成它。芒果从一开始就设计用来作为分布式数据库，处理多个节点是一个核心要求。使其在分片环境中要容易生成得多

## 分布式任务调度

### Quartz

- 使用数据库锁的方法执行job

### Elastic-job

### xxl-job

## 分布式事务

### 基础理论

- 本地事务

	- Spring事务@Transactional

		- [7种传播行为](https://juejin.im/entry/5a8fe57e5188255de201062b)

			- PROPAGATION_REQUIRED

				如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择

				- 拓展

					外围方法不开始事务：  
					如果两个function都用了相同传播行为，则其中一个事务的结果不会影响另外一个事务
					
					外围方法开启事务：  
					所有Propagation.REQUIRED修饰的内部方法和外围方法均属于同一事务，只要一个方法回滚0️⃣，整个事务均回滚
					
					0️⃣如果抛出异常的function没有try异常，而是留给调用它的对象try异常，即使catch没有抛出异常，那么将会回滚

			- PROPAGATION_SUPPORTS

				支持当前事务，如果当前没有事务，就以非事务方式执行

			- PROPAGATION_MANDATORY	

				使用当前的事务，如果当前没有事务，就抛出异常

			- PROPAGATION_REQUIRES_NEW

				新建事务，如果当前存在事务，把当前事务挂起

				- 拓展

					外围方法未开启事务：Propagation.REQUIRED修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰
					
					
					外围方法开启事务：Propagation.REQUIRES_NEW修饰的内部方法依然会单独开启独立事务，且与外部方法事务也独立，内部方法之间、内部方法和外部方法事务均相互独立，互不干扰

			- PROPAGATION_NOT_SUPPORTED

				以非事务方式执行操作，如果当前存在事务，就把当前事务挂起

			- PROPAGATION_NEVER

				以非事务方式执行，如果当前存在事务，则抛出异常

			- PROPAGATION_NESTED

				如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作

				- 拓展

					在外围方法未开启事务：  
					Propagation.NESTED和Propagation.REQUIRED作用相同，修饰的内部方法都会新开启自己的事务，且开启的事务相互独立，互不干扰
					
					外围方法开启事务：Propagation.NESTED修饰的内部方法属于外部事务的子事务，外围主事务回滚，子事务一定回滚，而内部子事务可以单独回滚而不影响外围主事务和其他子事务

		- 总结

			1.@Transactional基于AOP，要想生效，需要在异类调用
			
			2.只要事务链条的某个方法抛出异常，即使该异常被catch，事务回滚，比如dao层抛出异常，如果dao层直接catch，可以不回滚，如果service catch，事务回滚

	- ACID

- BASE

	- Basically Available(基本可用)

	- Soft state(软状态)允许系统某段时间数据不同步

	- Eventually consistent(最终一致)

- CAP

	- Consistency(一致性)，数据在各个node是一致的

	- Availability(可用性)，系统的性能

	- Partition tolerance(分区容错性)，指系统的可靠性，或者稳定性

- AKF

	Y 轴(功能) —— 关注应用中功能划分，基于不同的业务拆分
	X 轴(水平扩展) —— 关注水平扩展，也就是”加机器解决问题”，集群
	Z 轴(数据分区) —— 关注服务和数据的优先级划分，如按地域划分

### 幂等

同样的参数查询得到的结果永远一致  
select & delete操作天然幂等

- 阻止页面重复提交

	表单携带toekn提交，与服务器token对吧，如果一致删除token，返回新的token

- 全局唯一表示ID+redis

### [理论解决方案](https://www.toutiao.com/i6860672307614974477/)

- [2PC](https://www.toutiao.com/a6773252856997741060/)

	适用于数据库层面的分布式事务场景

	-  准备阶段（Prepare phase）

		事务管理器给每个参与者发送Prepare消息，每个数据库参与者在本地执行事务，并写本地的Undo/Redo日志，此时事务没有提交

	- 提交阶段（commit phase）

		如果事务管理器收到了参与者的执行失败或者超时消息时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据事务管理器的指令执行提交或者回滚操作，并释放事务处理过程中使用的锁资源。注意:必须在最后阶段释放锁资源

	- 问题

		- 失败重试

			第二阶段执行事务回滚操作：该阶段某些参与者回滚失败，只能一直重试，在所有参与者全部回滚之前，所有参与者都要阻塞  
			
			第二阶段执行提交操作：部分参与者提交失败，只能一直重试，直到所有都提交成功

		- 协调着单点问题

	- 落地方案

		- XA方案

			DTP模型定义TM和RM之间通讯的接口规范叫XA，简单理解为数据库提供的2PC接口协议，基于数据库的XA协议来实现2PC又称为XA方案

			- DTP模型

				1.应用程序（AP）持有用户库和积分库两个数据源
				
				2.应用程序（AP）通过TM通知用户库RM新增用户，同时通知积分库RM为该用户新增积分，RM此时并未提交事务，此时用户和积分资源锁定
				
				3.TM收到执行回复，只要有一方失败则分别向其他RM发起回滚事务，回滚完毕，资源锁释放
				
				4.TM收到执行回复，全部成功，此时向所有RM发起提交事务，提交完毕，资源锁释放

				- AP：应用程序

					使用DTP分布式事务的程序

				- RM：资源管理器

					事务的参与者，一般情况下是指一个数据库实例，通过资源管理器对该数据库进行控制，资源管理器控制着分支事务

				- TM：事务管理器

					负责协调和管理事务，事务管理器控制着全局事务，管理事务生命周期，并协调各个RM。全局事务是指分布式事务处理环境中，需要操作多个数据库共同完成一个工作，这个工作即是一个全局事务

			- 缺点

				1.性能问题：第一阶段和第二阶段所有的参与者资源和协调者资源均被锁定，参与者进行本地事务提交才释放资源，这期间对性能影响大
				
				2.单点故障：由于协调者的重要性，一旦 协调者 发生故障。参与者 会一直阻塞下去。尤其在第二阶段，协调者 发生故障，那么所有的 参与者 还都处于锁定事务资源的状态中，而无法继续完成事务操作。（虽然协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）

			- Jta实现

				作为java平台上事务规范JTA（Java Transaction API）也定义了对XA事务的支持，实际上，JTA是基于XA架构上建模的，在JTA 中，事务管理器抽象为javax.transaction.TransactionManager接口，并通过底层事务服务（即JTS）实现。像很多其他的java规范一样，JTA仅仅定义了接口，具体的实现则是由供应商(如J2EE厂商)负责提供，目前JTA的实现主要由以下几种：
				
				1.J2EE容器所提供的JTA实现(JBoss)
				2.独立的JTA实现:如JOTM，Atomikos.这些实现可以应用在那些不使用J2EE应用服务器的环境里用以提供分布事事务保证。如Tomcat,Jetty以及普通的java应用。

		- seate

			1.本地向TC注册的时候，把本地事务需要修改的数据table+pks提交到server端申请锁，拿到全局锁后，才能提交本地事务
			
			2.全局锁的结构resourceId + table + pks
			
			3.全局锁总结：操作一条记录的分支事务，必须等待这条记录的前一个分支事务执行结束（具体commit rollback情况分析如下），才能持有锁

			- AT模式

				- Transaction Coordinator (TC)： 事务协调器

					它是独立的中间件，需要独立部署运行，它维护全局事务的运行状态，接收TM指令发起全局事务的提交与回滚，负责与RM通信协调各各分支事务的提交或回滚

				- Transaction Manager (TM)： 事务管理器

					TM需要嵌入应用程序中工作，它负责定义事务范围（应该是事务保护哪几个服务），开启/提交/回滚事务

				- Resource Manager (RM)： 控制分支事务

					负责分支注册、状态汇报，并接收事务协调器TC的指令，驱动分支（本地）事务的提交和回滚

				- 案例

					1.用户服务的 TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的XID
					
					2.用户服务的 RM 向 TC 注册 分支事务，该分支事务在用户服务执行新增用户逻辑，并将其纳入 XID 对应全局事务的管辖
					
					3.用户服务执行分支事务，向用户表插入一条记录  
					
					4.逻辑执行到远程调用积分服务时(XID 在微服务调用链路的上下文中传播)。积分服务的RM 向 TC 注册分支事务，该分支事务执行增加积分的逻辑，并将其纳入 XID 对应全局事务的管辖
					
					5.积分服务执行分支事务，向积分记录表插入一条记录，执行完毕后，返回用户服务  
					
					6.用户服务分支事务执行完毕  
					
					7.TM 向 TC 发起针对 XID 的全局提交或回滚决议
					
					8.TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求

				- Seata实现2PC与传统2PC的差别

					架构层次方面，传统2PC方案的 RM 实际上是在数据库层，RM 本质上就是数据库自身，通过 XA 协议实现，而Seata的 RM 是以jar包的形式作为中间件层部署在应用程序这一侧的。
					
					两阶段提交方面，传统2PC无论第二阶段的决议是commit还是rollback，事务性资源的锁都要保持到Phase2完成才释放。而Seata的做法是在Phase1 就将本地事务提交，这样就可以省去Phase2持锁的时间，整体提高效率
					
					对业务无入侵（虽然XA也是）

			- MT模式

			- AT%MT模式差异

				1.TA模式xid由上游获取，传递到下游
				
				2.MT模式

			- [源码分析](https://zhuanlan.zhihu.com/p/63381854)

				- ConnectionProxy

					- 下游事务被其代理，原业务commit方法被拦截

				- @GlobalTransactional

			- 隔离性和锁分析

- 3PC

	- CanCommit（事务询问）

		尝试获取数据库锁 如果可以，就返回Yes

	- PreCommit（事务执行）

		PreCommit阶段跟第一阶段差不多的，只不过这里协调者和参与者都引入了超时机制

	- DoCommit（事务提交）

		2pc的阶段二是差不多的

	- 3PC比2PC好在哪

		- 降低同步阻塞

			在3PC中，第一阶段并没有让参与者直接执行事务，而是在第二阶段才会让参与者进行事务的执行。大大降低了阻塞的概率和时长。并且，在3PC中，如果参与者未收到协调者的消息，那么他会在等待一段时间后自动执行事务的commit，而不是一直阻塞

		- 提高数据一致性

			2PC中有一种情况会导致数据不一致，如在2PC的阶段二中，当协调者向参与者发送commit请求之后，发生了网络异常，只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。
			
			这种情况在3PC的场景中得到了很好的解决，因为在3PC中，如果参与者没有收到协调者的消息时，他不会一直阻塞，过一段时间之后，他会自动执行事务。这就解决了那种协调者发出commit之后。
			
			另外，2PC还有个问题无法解决。那就是协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。
			
			这种情况在3PC中是有办法解决的，因为在3PC中，选出新的协调者之后，他可以咨询所有参与者的状态，如果有某一个处于commit状态或者prepare-commit状态，那么他就可以通知所有参与者执行commit，否则就通知大家rollback。因为3PC的第三阶段一旦有机器执行了commit，那必然第一阶段大家都是同意commit的，所以可以放心执行commit

	- 3PC无法解决的问题

		在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者abort请求时，会在等待超时之后，会继续进行事务的提交。
		
		所以，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。
		
		所以，我们可以认为，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题

- TCC

	2PC 和 3PC 都是数据库层面的，而 TCC 是业务层面的分布式事务  
	
	例子：A服务转30块钱、B服务转50块钱，一起到C服务上。  
	
	Try：尝试执行业务。完成所有业务检查(一致性)：检查A、B、C的帐户状态是否正常，帐户A的余额是否不少于30元，帐户B的余额是否不少于50元。预留必须业务资源  
	
	(准隔离性)：帐户A的冻结金额增加30元，帐户B的冻结金额增加50元，这样就保证不会出现其他并发进程扣减了这两个帐户的余额而导致在后续的真正转帐操作过程中，  
	
	帐户A和B的可用余额不够的情况。  
	
	Confirm：确认执行业务。真正执行业务：如果Try阶段帐户A、B、C状态正常，且帐户A、B余额够用，则执行帐户A给账户C转账30元、帐户B给账户C转账50元的转帐  
	
	操作。 这时已经不需要做任何业务检查，Try阶段已经完成了业务检查。只使用Try阶段预留的业务资源：只需要使用Try阶段帐户A和帐户B冻结的金额即可。  
	
	Cancel：取消执行业务释放Try阶段预留的业务资源：如果Try阶段部分成功，比如帐户A的余额够用，且冻结相应金额成功，帐户B的余额不够而冻结失败，则需要  
	
	对帐户A做Cancel操作，将帐户A被冻结的金额解冻掉。

	- Try

		Try操作是先把多个应用中的业务资源预留和锁定住，为后续的确认打下基础，类似的，DML操作要锁定数据库记录行，持有数据库资源

	- Confirm

		Confirm操作是在Try操作中涉及的所有应用均成功之后进行确认，使用预留的业务资源，和Commit类似。Confirm操作满足幂等性。要求具备幂等设计，Confirm失败后需要进行重试。

	- Cancel

		Cancel则是当Try操作中涉及的所有应用没有全部成功，需要将已成功的应用进行取消(即Rollback回滚)。其中Confirm和Cancel操作是一对反向业务操作

		- 

	- 落地方案

		- GTS

			- GTS开源版Fescar

	- TCC和2PC区别

		2PC是资源层面的分布式事务，强一致性，在两阶段提交的整个过程中，一直会持有资源的锁。
		
		XA事务中的两阶段提交内部过程是对开发者屏蔽的，事务管理器在两阶段提交过程中，从prepare到commit/rollback过程中，资源实际上一直都是被加锁的。
		
		如果有其他人需要更新这两条记录，那么就必须等待锁释放。  
		
		TCC是业务层面的分布式事务，最终一致性，不会一直持有资源的锁。
		
		我的理解就是当执行try接口的时候，已经把所需的资源给预扣了，比如上面举例的A服务已经预扣30元，B服务已经预扣50元，它是由try接口实现，这样就保证不会出现其他并发进程扣减了这两个帐户的余额而导致在后续的真正转帐操作过程中，帐户A和B的可用余额不够的情况，同时保证不会一直锁住整个资源。（核心点应该就在这）
		
		TCC中的两阶段提交并没有对开发者完全屏蔽，也就是说从代码层面，开发者是可以感受到两阶段提交的存在。
		
		1、try过程的本地事务，是保证资源预留的业务逻辑的正确性。
		
		2、confirm/cancel执行的本地事务逻辑确认/取消预留资源，以保证最终一致性，也就是所谓的补偿型事务。

- 消息服务

	- 最大努力通知型

		1.A系统执行成功
		
		2.消息投递到MQ  
		
		3.B系统执行与否不影响事务最终结果  
		
		适用：通知服务

	- 独立消息服务

		1、消息服务子系统：
		a、用于存储预发送消息
		b、确认并发送消息
		c、查询状态确认超时的消息
		d、确认消息已被成功消费
		e、查询消息确认超时的消息
		f、删除本地消息（已完成）
		2、MQ实时消息服务子系统：mq消息队列  
		3、消息状态确认子系统：用于轮询 or 定时任务查询生产方消息投递异常的记录，进行重新投递  
		4、消息恢复子系统：用于轮询 or 定时任务查询 消费端消费失败的异常日志记录，进行消费恢复

	- 本地消息服务

		基于数据库，效率低

- Paxos算法

- SAGAS模型

	拆分事务，一个调一个，做好补偿

- LCN

## 配置中心

## 分布式数据存储

### 分库分表

- 水平

	- 水平分库

		- 其表现和水平分表相差无几，只是将这些拆分的数据表分散到不同数据库

	- 水平分表

		- 从某行截取数据分散到多个不同的数据表，这些数据表字段保持一致。库级别IO瓶颈依然存在，不建议使用

- 垂直

	- 垂直分库

		- 根据业务不同拆分不同数据库，订单一个库，用户一个库，商品一个库

	- 垂直分表

		- 大表拆小表，拆分的是基于关系型数据库的列

- 跨库跨表SQL

	- 同实例-Instance

		- sql的时候加上库名即可

	- 不同实例-non-instance

		- 通过FEDERATED存储引擎，在本地创建一个跟远端库表结构一模一样的表来实现. 有点类似Oracle中的数据库链接(DBLINK)

			- 缺点：不支持事务，无法获知远端表结构改变

- 扩容方案

	- 一致HASH并做数据迁移

### 主从复制(Master-Slave)

- 主从同步机制

	- Mysql中有一种日志叫做bin日志(二进制日志)。这个日志会记录下所有修改了数据库的SQL语句，其原理其实就是把
	  主服务器上的bin日志复制到从服务器上执行一遍，这样从服务器上的数据就和主服务器上的数据相同了

- 读写分离

	MySQL读写分离基本原理是让master数据库处理写操作，slave数据库处理读操作。master将写操作的变更同步到各个slave节点

	- 基于程序代码内部实现

		- 在代码中根据select 、insert进行路由分类，这类方法也是目前生产环境下应用
		  最广泛的。优点是性能较好，因为程序在代码中实现，不需要增加额外的硬件开  
		  支，缺点是需要开发人员来实现，运维人员无从下手(需要配置从库选择策略)

### 多元数据库路由

- AbstractRoutingDataSource(springJdbc)

	- 被DynamicDataSource继承，内部维护targetDataSource的map，重写determineCurrentLookupKey()，此方法决定具体从哪个数据源获取连接

### 同构&异构数据库

### 中间件

- 作用于代理层-mycat

	重新编写了一个JDBC的驱动，在内存中维护一个路由列表，然后将请求转发到真正的数据库连接中

	- 1.可以负责更多的内容，将数据迁移，分布式事务等纳入Proxy的范畴
	  2.更有效的管理数据库的连接
	  3.整合大数据思路，将OLTP和OLAP分离处理

- [作用于驱动层-sharding-jdbc](https://blog.csdn.net/shijiemozujiejie/article/details/80786231)

	客户端直连数据库，以jar包形式提供服务，支持TA事务和柔性事务，未使用中间层

	- 1.轻量，范围更加容易界定，只是JDBC增强，不包括HA、事务以及数据库元数据管理
	  2.开发的工作量较小，无需关注nio，各个数据库协议等
	  3.运维无需改动，无需关注中间件本身的HA
	  4.性能高，JDBC直连数据库，无需二次转发
	  5.可支持各种基于JDBC协议的数据库，如：MySQL、Oralce、SQLServer

## 服务治理

### RPC

RPC调用过程：  
1.获取远端服务地址  
2.请求参数序列化，用于TCP连接发送到目标服务器  
3.目标服务器反序列化请求参数，调用本地服务，返回序列化的结果  
4.获取返回结果并反序列化

- 原理

	- 用到的技术

		- 动态代理

		- 序列化&反序列化

		- NIO通信

		- 服务注册中心

	- 传输协议

		- grpc

			基于Google开发的RPC协议，使用http/2，序列化使用protobuf

			- http/2

		- 和http的区别

	- 序列化协议

		- protobuf

			Protobuf实际是一套类似Json或者XML的数据传输格式和规范，用于不同应用或进程之间进行通信时使用。通信时所传递的信息是通过Protobuf定义的message数据结构进行打包，然后编译成二进制的码流再进行传输或者存储。
			
			足够简单  
			序列化后体积很小:消息大小只需要XML的1/10 ~ 1/3
			解析速度快:解析速度比XML快20 ~ 100倍
			多语言支持  
			更好的兼容性,Protobuf设计的一个原则就是要能够很好的支持向下或向上兼容

			- 序列化字节码最小

		- hessian

		- xml

		- json

		- Kryo

			- 序列化性能最高

- 主流框架

	- Dubbo

		透明化的远程方法调用，就像调用本地方法一样调用远程方法，只需简单配置，没有任何API侵入
		
		软负载均衡及容错机制，可在内网替代F5等硬件负载均衡器，降低成本，减少单点  
		
		服务自动注册与发现，不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者

		- 架构

			- Container

				- Provider

					- 失效踢出原理(基于ZK)

					- 1.服务提供者在启动时，向注册中心注册自己提供的服务

				- 0.服务容器负责启动，加载，运行服务提供者

			- Consumer

				- 2.服务消费者在启动时，向注册中心订阅自己所需的服务

				- 4.服务消费者，从提供者地址列表中，基于软负载均衡算法（负载均衡算法包括轮询法、随机法、最少活跃调用数、一致性Hash等），选一台提供者进行调用，如果调用失败，再选另一台调用

					- 负载均衡算法

						- Random  LoadBalance

							 随机，按权重设置随机概率。
							      在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重

						- RoundRobin  LoadBalance

							轮循，按公约后的权重设置轮循比率。  
							      存在慢的提供者累积请求问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上

						- LeastActive  LoadBalance

							  最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。
							      使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大

						- ConsistentHash  LoadBalance

							 一致性Hash，相同参数的请求总是发到同一提供者。
							      当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动

				- 补充：消费者缓存调用过的服务，在注册中心无法使用的情况下，依然可以消费服务

			- Registry

				- 3.注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者

				- 补充：注册中心，服务提供者，服务消费者三者之间均为长连接。服务消费者可以直连服务提供者

			- Monitor

				- 5.服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心

		- 调用方式

			- 同步调用

				- 阻塞式的调用方式，消费者一直阻塞等待，直到服务端返回为止

				- Dubbo的底层IO操作都是异步的。消费者发起调用后，得到一个 Future 对象，在失效时间内监听返回结果，并根据结果作出反应

			- 异步调用

			- 参数回调

			- 事件通知

		- 客户端和服务端通信：基于netty

			- NettyHandler进行具体的消息收发操作

		- 集群容错模式

			- Failover Cluster

				失败自动切换，当出现失败，重试其它服务器。(缺省)
				通常用于读操作，但重试会带来更长延迟  
				可通过retries="2"来设置重试次数(不含第一次)。正是文章刚开始说的那种情况

			- Failfast Cluster

				快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录

			- Failsafe Cluster

				失败安全，出现异常时，直接忽略。  
				通常用于写入审计日志等操作

			- Failback Cluster

				失败自动恢复，后台记录失败请求，定时重发。  
				通常用于消息通知操作

			- Forking Cluster

				并行调用多个服务器，只要一个成功即返回。  
				通常用于实时性要求较高的读操作，但需要浪费更多服务资源。  
				可通过forks="2"来设置最大并行数

			- Broadcast Cluster

				广播调用所有提供者，逐个调用，任意一台报错则报错。(2.1.0开始支持)
				通常用于通知所有提供者更新缓存或日志等本地资源信息

		- 安全机制

			- 通过Token令牌防止用户绕过注册中心直连，然后在注册中心上管理授权。Dubbo还提供服务黑白名单，来控制服务所允许的调用方

		- 通讯协议

			rmi： 采用JDK标准的rmi协议实现，传输参数和返回参数对象需要实现Serializable接口，使用java标准序列化机制，使用阻塞式短连接，传输数据包大小混合，消费者和提供者个数差不多，可传文件，传输协议TCP。 多个短连接，TCP协议传输，同步传输，适用常规的远程服务调用和rmi互操作。在依赖低版本的Common-Collections包，java序列化存在安全漏洞；
			
			webservice： 基于WebService的远程调用协议，集成CXF实现，提供和原生WebService的互操作。多个短连接，基于HTTP传输，同步传输，适用系统集成和跨语言调用；
			http： 基于Http表单提交的远程调用协议，使用Spring的HttpInvoke实现。多个短连接，传输协议HTTP，传入参数大小混合，提供者个数多于消费者，需要给应用程序和浏览器JS调用；
			
			hessian： 集成Hessian服务，基于HTTP通讯，采用Servlet暴露服务，Dubbo内嵌Jetty作为服务器时默认实现，提供与Hession服务互操作。多个短连接，同步HTTP传输，Hessian序列化，传入参数较大，提供者大于消费者，提供者压力较大，可传文件；
			
			memcache： 基于memcached实现的RPC协议
			
			redis： 基于redis实现的RPC协议

			- dubbo协议

				Dubbo缺省协议采用单一长连接和NIO异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。Dubbo缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。
				缺省协议，使用基于mina1.1.7+hessian3.2.1实现。
				连接个数：单连接  
				连接方式：长连接  
				传输协议：TCP
				传输方式：NIO异步传输
				序列化：Hessian二进制序列化

			- rmi协议

				RMI协议采用JDK标准的Java.rmi.*实现，采用阻塞式短连接和JDK标准序列化方式。
				如果服务接口继承了java.rmi.Remote接口，可以和原生RMI互操作，即：
				提供者用Dubbo的RMI协议暴露服务，消费者直接用标准RMI接口调用，
				或者提供方用标准RMI暴露服务，消费方用Dubbo的RMI协议调用。
				如果服务接口没有继承java.rmi.Remote接口，
				缺省Dubbo将自动生成一个com.xxx.XxxService$Remote的接口，并继承java.rmi.Remote接口，并以此接口暴露服务，
				但如果设置了<dubbo:protocol name="rmi" codec="spring"/>，将不生成$Remote接口，而使用Spring的RmiInvocationHandler接口暴露服务，和Spring兼容。

			- hessian协议

				Hessian协议用于集成Hessian的服务，Hessian底层采用Http通讯，采用Servlet暴露服务，Dubbo缺省内嵌Jetty作为服务器实现。
				Hessian是Caucho开源的一个RPC框架：http://hessian.caucho.com，其通讯效率高于WebService和Java自带的序列化

			- http协议

				此协议采用spring 的HttpInvoker的功能实现，
				连接个数：多个  
				连接方式：长连接  
				连接协议：http
				传输方式：同步传输  
				序列化：表单序列化  
				适用范围：传入传出参数数据包大小混合，提供者比消费者个数多，可用浏览器查看，可用表单或URL传入参数，暂不支持传文件。
				适用场景：需同时给应用程序和浏览器js使用的服务

			- webservice协议

			- thrift协议

				当前 dubbo 支持的 thrift 协议是对 thrift 原生协议的扩展，在原生协议的基础上添加了一些额外的头信息，比如service name，magic number等。使用dubbo thrift协议同样需要使用thrift的idl compiler编译生成相应的java代码

			- memcached协议

			- redis协议

	- Thrift：跨语言

	- RMI

		RMI使用的是JRMP（JAVA Remote Messageing Protocol），可以说JRMP是专门为java定制的通信协议，所以它是纯java的分布式解决方案

		- SOAP

	- Feign

		- HTTP

	- Netty

		- TCP

	- WebService

		- XML—RPC

### SOAP

- HTTP+XML

### REST

- 基于HTTP协议的直接调用

## 分布式搜索引擎

### lucene

Apache开源工具包

- 倒排索引-产生

	Java is the best programming language.  
	PHP is the best programming language.  
	Javascript is the best programming language.  
	
	核心词汇：  
	词条（Term）：索引里面最小的存储和查询单元，对于英文来说是一个单词，对于中文来说一般指分词后的一个词。  
	
	词典（Term Dictionary）：或字典，是词条 Term 的集合。搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。  
	
	倒排表（Post list）：一个文档通常由多个词组成，倒排表记录的是某个词在哪些文档里出现过以及出现的位置。  
	
	每条记录称为一个倒排项（Posting）。倒排表记录的不单是文档编号，还存储了词频等信息。  
	
	倒排文件（Inverted File）：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件被称之为倒排文件，倒排文件是存储倒排索引的物理文件。

	- 词典

	- 倒排文件

### [el](https://zhuanlan.zhihu.com/p/33375126)a[sticSearch](https://zhuanlan.zhihu.com/p/33375126)

基于lucene开发，具有分布式的特性和易安装的特点，solr虽然也基于lucene，但搭建分布式需要ZK等第三方协调管理实现

- 集群模式

	- 发现机制

		- Zen Discovery

	- 分片

- CURD

	- 写操作

		- 1.索引新文档（Create）

			当用户向一个节点提交了一个索引新文档的请求，节点会计算新文档应该加入到哪个分片（shard）中。每个节点都存储有每个分片存储在哪个节点的信息，因此协调节点会将请求发送给对应的节点。注意这个请求会发送给主分片，等主分片完成索引，会并行将请求发送到其所有副本分片，保证每个分片都持有最新数据。
			
			每次写入新文档时，都会先写入内存中，并将这一操作写入一个translog文件（transaction log）中，此时如果执行搜索操作，这个新文档还不能被索引到。

		- 2.新文档被写入内存，操作被写入translog

			ES会每隔1秒时间（这个时间可以修改）进行一次刷新操作（refresh），此时在这1秒时间内写入内存的新文档都会被写入一个文件系统缓存（filesystem cache）中，并构成一个分段（segment）。此时这个segment里的文档可以被搜索到，但是尚未写入硬盘，即如果此时发生断电，则这些文档可能会丢失

		- 3.在执行刷新后清空内存，新文档写入文件系统缓存

			不断有新的文档写入，则这一过程将不断重复执行。每隔一秒将生成一个新的segment，而translog文件将越来越大

		- 4.translog不断加入新文档记录

			每隔30分钟或者translog文件变得很大，则执行一次fsync操作。此时所有在文件系统缓存中的segment将被写入磁盘，而translog将被删除（此后会生成新的translog）

		- 5.执行fsync后segment写入磁盘，清空内存和translog

			由上面的流程可以看出，在两次fsync操作之间，存储在内存和文件系统缓存中的文档是不安全的，一旦出现断电这些文档就会丢失。所以ES引入了translog来记录两次fsync之间所有的操作，这样机器从故障中恢复或者重新启动，ES便可以根据translog进行还原。
			
			当然，translog本身也是文件，存在于内存当中，如果发生断电一样会丢失。因此，ES会在每隔5秒时间或是一次写入请求完成后将translog写入磁盘。可以认为一个对文档的操作一旦写入磁盘便是安全的可以复原的，因此只有在当前操作记录被写入磁盘，ES才会将操作成功的结果返回发送此操作请求的客户端。
			
			此外，由于每一秒就会生成一个新的segment，很快将会有大量的segment。对于一个分片进行查询请求，将会轮流查询分片中的所有segment，这将降低搜索的效率。因此ES会自动启动合并segment的工作，将一部分相似大小的segment合并成一个新的大segment。合并的过程实际上是创建了一个新的segment，当新segment被写入磁盘，所有被合并的旧segment被清除

		- 6.合并segment，删除旧segment，新segment可供搜索

	- 查询

		- 分布式搜索

			第一步是广播请求到索引中每一个节点的分片拷贝。 查询请求可以被某个主分片或某个副本分片处理，协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。
			
			每个分片将会在本地构建一个优先级队列。如果客户端要求返回结果排序中从第from名开始的数量为size的结果集，则每个节点都需要生成一个from+size大小的结果集，因此优先级队列的大小也是from+size。分片仅会返回一个轻量级的结果给协调节点，包含结果集中的每一个文档的ID和进行排序所需要的信息。
			
			协调节点会将所有分片的结果汇总，并进行全局排序，得到最终的查询排序结果。此时查询阶段结束

		- 获取搜索结果

			查询过程得到的是一个排序结果，标记出哪些文档是符合搜索要求的，此时仍然需要获取这些文档返回客户端。  
			
			协调节点会确定实际需要返回的文档，并向含有该文档的分片发送get请求；分片获取文档返回给协调节点；协调节点将结果返回给客户端

	- 更新和删除文档

		ES的索引是不能修改的，因此更新和删除操作并不是直接在原索引上直接执行。
		
		每一个磁盘上的segment都会维护一个del文件，用来记录被删除的文件。每当用户提出一个删除请求，文档并没有被真正删除，索引也没有发生改变，而是在del文件中标记该文档已被删除。因此，被删除的文档依然可以被检索到，只是在返回检索结果时被过滤掉了。每次在启动segment合并工作时，那些被标记为删除的文档才会被真正删除。
		
		更新文档会首先查找原文档，得到该文档的版本号。然后将修改后的文档写入内存，此过程与写入一个新文档相同。同时，旧版本文档被标记为删除，同理，该文档可以被搜索到，只是最终被过滤掉

- [倒排索引](https://zhuanlan.zhihu.com/p/33671444)

	- Term

		- 针对源数据分词出来的单个词

	- Term Dictionary

		- 由T-Tree存储排序后的Term形成

	- Posting List

		- Roaring bitmaps压缩

			将posting list按照65535为界限分块，将id➗(65535+1)，以<商，余数>的结构将其存储，控制ID在有限(0~65335)范围内

			- 为什么是以65535为界限

				因为它=2^16-1，正好是用2个字节能表示的最大数，一个short的存储单位。注意图里的最后一行“If a block has more than 4096 values, encode as a bit set, and otherwise as a simple array using 2 bytes per value”，如果是大块，用节省点用bitset存，小块就豪爽点，2个字节我也不计较了，用一个short[]存着方便

			- 为什么用4096来区分大小块

				4096*2bytes ＝ 8192bytes < 1KB, 磁盘一次寻道可以顺序把一个小块的内容都读出来，再大一位就超过1KB了，需要两次读

		- 增量编码压缩(lucene老版本)

			第一步： 增量编码就是从第二个数开始每个数存储与前一个id的差值，即300-73=227，302-300=2，...，一直到最后一个数；
			第二步： 就是将这些差值放到不同的区块，Lucene使用256个区块，下面示例为了方便展示使用了3个区块，即每3个数一组；
			第三步： 位压缩，计算每组3个数中最大的那个数需要占用bit位数，比如30、11、29中最大数30最小需要5个bit位存储，这样11、29也用5个bit位存储，这样才占用15个bit，不到2个字节，压缩效果很好。

		- 倒排索引，以int数组存储Term等位置信息

	- Term Index

		- 存储Term等前缀，通过它可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找

## 分布式日志系统

### [ELK](https://www.toutiao.com/i6746004513028571659/)

- ElsticSearch

- Kibana

- [logStash](https://www.cnblogs.com/cjsblog/p/9459781.html)

	同时从多个数据源获取数据，并对其进行转换，然后将其发送到你最喜欢的“存储”

	- Filebeat

		- 从服务器收集并发送日志行到Logstash，与logStash分开搭建

	- input

	- filter

	- output

### logback：springboot默认

- [配置文件：logback.xml](https://juejin.im/post/5b51f85c5188251af91a7525)

	想使用spring扩展profile支持，要以logback-spring.xml命名，其他如property需要改为springProperty

	- appender

	- logger

	- root

### log4j

### slf4j：java顶级接口

- 日志级别

	- OFF、FATAL、ERROR、WARN、INFO、DEBUG、TRACE、 ALL

## 分布式部署节点问题

### 扩容问题

- 一致性Hash

	保障集群扩展及容错能力，将机器对2^32取莫，部署到环形hash表，避免机器数量变化时，缓存全部失效

## 分布式限流

### [限流算法](https://juejin.im/post/6844904090309246989)

- 漏桶

	存下请求  
	匀速处理  
	多于丢弃  
	
	为了消除"突刺现象"，可以采用漏桶算法实现限流，漏桶算法这个名字就很形象，算法内部有一个容器，类似生活用到的漏斗，当请求进来时，相当于水倒入漏斗，然后从下端小口慢慢匀速的流出。不管上面流量多大，下面流出的速度始终保持不变。  
	
	不管服务调用方多么不稳定，通过漏桶算法进行限流，每10毫秒处理一次请求。因为处理的速度是固定的，请求进来的速度是未知的，可能突然进来很多请求，没来得及处理的请求就先放在桶里，既然是个桶，肯定是有容量上限，如果桶满了，那么新进来的请求就丢弃。  
	
	在算法实现方面，可以准备一个队列，用来保存请求，另外通过一个线程池定期从队列中获取请求并执行，可以一次性获取多个并发执行。  
	
	这种算法，在使用过后也存在弊端：无法应对短时间的突发流量。

- 令牌桶

	1.系统以恒定的速率产生令牌，然后将令牌放入令牌桶中  
	2.令牌桶有一个容量，当令牌桶满了的时候，再向其中放入的令牌就会被丢弃  
	3.每次一个请求过来，需要从令牌桶中获取一个令牌，假设有令牌，那么提供服务；假设没有令牌，那么拒绝服务

	- [guava RateLimiter](https://www.toutiao.com/i6791468953957827086/)

- 滑动窗口计数器

	将时间划分为多个区间；  
	在每个区间内每有一次请求就将计数器加一维持一个时间窗口，占据多个区间；  
	每经过一个区间的时间，则抛弃最老的一个区间，并纳入最新的一个区间；  
	如果当前窗口内区间的请求计数总和超过了限制数量，则本窗口内所有的请求都被丢弃

	- redis解决方案

		利用redis的zset，将每次请求的时间戳作为scope，并设置过期时间，当请求过来的时候，获取当前时间，当前时间减去窗口长度，抽取该区间请求数量，超过限制就丢弃

- 固定窗口计数器算法

	// 限流的个数
	    private int maxCount = 10;  
	    // 指定的时间内
	    private long interval = 60;  
	    // 原子类计数器
	    private AtomicInteger atomicInteger = new AtomicInteger(0);  
	    // 起始时间
	    private long startTime = System.currentTimeMillis();  
	
	    public boolean limit(int maxCount, int interval) {  
	        atomicInteger.addAndGet(1);  
	        if (atomicInteger.get() == 1) {  
	            startTime = System.currentTimeMillis();  
	            atomicInteger.addAndGet(1);  
	            return true;  
	        }  
	        // 超过了间隔时间，直接重新开始计数
	        if (System.currentTimeMillis() - startTime > interval * 1000) {  
	            startTime = System.currentTimeMillis();  
	            atomicInteger.set(1);  
	            return true;  
	        }  
	        // 还在间隔时间内,check有没有超过限流的个数
	        if (atomicInteger.get() > maxCount) {  
	            return false;  
	        }  
	        return true;  
	    }

	- 风险

		在前一个周期后半段和后一个后期前半段的时间段内，请求存在超过服务器极限的风险

### 单机版

基于自定义注解+AOP+guava包下的ratelimiter

- 切面

	@Log4j2  
	@Aspect  
	@Configuration  
	public class RateLimitAspect {  
	
	    private RateLimiter rateLimiter = RateLimiter.create(Double.MAX_VALUE);  
	
	    @Around("execution(public * *(..)) && @annotation(com.yuan.redis.authorization.RateLimit)")  
	    public Object interceptor(ProceedingJoinPoint pjp) throws Throwable {  
	        MethodSignature signature = (MethodSignature) pjp.getSignature();  
	        Method method = signature.getMethod();  
	        if (method.isAnnotationPresent(RateLimit.class)) {  
	            RateLimit rl = method.getAnnotation(RateLimit.class);  
	            rateLimiter.setRate(rl.perSecond());  
	            if (!rateLimiter.tryAcquire(rl.timeOut(), rl.timeOutUnit())) {  
	                throw new ApiException("访问人数太多,请稍后再试试", ApiConstants.ERROR100600);
	            }  
	        }  
	        return pjp.proceed();  
	    }  
	}

- 自定义注解

	@Target({ElementType.METHOD})  
	@Retention(RetentionPolicy.RUNTIME)  
	public @interface RateLimit {  
	
	
	    /**  
	     * 每秒向桶放入令牌的数量
	     */  
	     double perSecond() default Double.MAX_VALUE;  
	
	    /**  
	     * 获取令牌的等待时间
	     *  
	     * @return  
	     */  
	    int timeOut() default 0;  
	
	    /**  
	     * 超时时间单位
	     *  
	     * @return  
	     */  
	    TimeUnit timeOutUnit() default  TimeUnit.MILLISECONDS;  
	
	}

- 测试接口

	    @ApiOperation(value = "测试限流", notes = "测试限流")
	    @RequestMapping(value = "/testRateLimiter.json", method = RequestMethod.POST)  
	    @ApiResponses({@ApiResponse(code = 5000001, message = "参数错误")})
	    @RateLimit(perSecond =100, timeOut = 100)  
	    public Result<Test>  testRateLimiter() {  
	        return Result.jsonStringOk();  
	    }

### 分布式版本

- [Redis + Lua](https://zhuanlan.zhihu.com/p/85166364)

## 分布式熔断

## 分布式锁

### 四个要点

1.互斥性：在任意时刻，只有一个客户端能持有锁  

2.不会发生死锁：即使一个客户端持有锁的期间崩溃而没有主动释放锁，也需要保证后续其他客户端能够加锁成功  

3.加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给释放了  

4.容错性：只要大部分的Redis节点正常运行，客户端就可以进行加锁和解锁操作

### 基于数据库

- 乐观锁

	- 获取数据的时候不加锁，到了更新数据的时候对某个字段进行比较

		Update user set password = #password where id = #id and #version = version

- 悲观锁

	- 共享锁：使用排他写锁，不允许其他线程访问被锁数据行。注意，条件column如果没有添加索引，会锁表

		selete * from user where id = #id lock in share mode  
		select * from user where id = #id for update

### [基于Redis](https://www.jianshu.com/p/268e5d4ce045)

- 单机版

	- set

		set(final String key, final String value, final String nxxx, final String expx, final int time)   
		
		(1)key，我们使用key来当锁，key是唯一的。
		
		(2)value，我们传的是“时间+请求号”，通过给value赋值我们在解锁的时候就会传递同样的数据进行解锁。不至于出现不同的服务器对key进行解锁
		
		(3)nxxx，NX意思为SET IF NOT EXIST，即当key不存在时，我们进行set操作；若key已经存在，则不做任何操作；
		
		(4)expx，PX意思是给这个key加一个过期设置，具体时间由第五个参数决定。
		
		(5)time，代表key的过期时间，单位毫秒。

	- setNx

	- 加锁解锁原则

		1.保证原子性，lua脚本
		
		2.保证key唯一  
		
		3.保证value的唯一标示性，分布式系统下可以用机器ID做value  
		
		4.当一个服务器向redis加锁时候，我们需要确定这个key是来自于哪台服务器，在解锁时需要校验是不是解锁的请求来自于同一个服务器  
		
		5.解锁时用lua脚本保证原子性

	- 加锁

		String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime)

	- 解锁

		String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";  
		        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));

	- 失败问题

		问题描述：客户端因为业务执行超时导致锁失效，另一个客户端趁机获得锁，同一段代码逻辑同时有两个线程在执行，违背排他锁原则  
		
		解决方案：另起一个线程检测还没解锁的key是否临近超时，只要客户端还在（还活着），且key的值是该客户端一开始设置的值，就可以帮其它延长key有效时间。  
		
		注意：如果无法延长key，那说明key过期了，客户端必须重新持有锁。要控制好线程轮训间隔和key延长时间，避免出现一个轮训周期内出现刚key过期就轮训  
		
		if redis.call("get",KEYS[1]) == ARGV[1] then  
		
		          redis.call("set",KEYS[1],ex=3000)    
		
		else  
		
		          getDLock();//重新获取锁    

- 主从版

	网上讲的基于故障转移实现的redis主从无法真正实现Redlock:
	
	因为redis在进行主从复制时是异步完成的，比如在clientA获取锁后，主redis复制数据到从redis过程中崩溃了，导致没有复制到从redis中，然后从redis选举出一个升级为主redis,造成新的主redis没有clientA 设置的锁，这是clientB尝试获取锁，并且能够成功获取锁，导致互斥失效；
	
	思考题：这个失败的原因是因为从redis立刻升级为主redis，如果能够过TTL时间再升级为主redis（延迟升级）后，或者立刻升级为主redis但是过TTL的时间后再执行获取锁的任务，就能成功产生互斥效果；是不是这样就能实现基于redis主从的Redlock;

- 集群版

	- redlock算法

		作用：补充单机版分布式锁单点问题  
		
		假设有5个完全独立的redis主服务器：  
		
		1.获取当前时间戳  
		
		2.client尝试按照顺序使用相同的key,value获取所有redis服务的锁，在获取锁的过程中的获取时间比锁过期时间短很多，这是为了不要过长时间等待已经关闭的redis服务。并且试着获取下一个redis实例。  
		
		比如：TTL为5s,设置获取锁最多用1s，所以如果一秒内无法获取锁，就放弃获取这个锁，从而尝试获取下个锁  
		
		3.client通过获取所有能获取的锁后的时间减去第一步的时间，这个时间差要小于TTL时间并且至少有3个redis实例成功获取锁，才算真正的获取锁成功  
		
		4.如果成功获取锁，则锁的真正有效时间是 TTL减去第三步的时间差 的时间；比如：TTL 是5s,获取所有锁用了2s,则真正锁有效时间为3s(其实应该再减去时钟漂移);  
		
		5.如果客户端由于某些原因获取锁失败，便会开始解锁所有redis实例；因为可能已经获取了小于3个锁，必须释放，否则影响其他client获取锁

		- 崩溃问题

			1.如果redis没有持久化功能，在clientA获取锁成功后，某个redis重启，clientB能够再次获取到锁，这样违法了锁的排他互斥性;
			
			2.如果启动AOF永久化存储，事情会好些， 举例:当我们重启redis后，由于redis过期机制是按照unix时间戳走的，所以在重启后，然后会按照规定的时间过期，不影响业务;但是由于AOF同步到磁盘的方式默认是每秒-次，如果在一秒内断电，会导致数据丢失，立即重启会造成锁互斥性失效;但如果同步磁盘方式使用Always(每一个写命令都同步到硬盘)造成性能急剧下降;所以在锁完全有效性和性能方面要有所取舍; 
			
			3.有效解决既保证锁完全有效性及性能高效及即使断电情况的方法是redis同步到磁盘方式保持默认的每秒，在redis无论因为什么原因停掉后要等待TTL时间后再重启(学名:延迟重启) ;缺点是 在TTL时间内服务相当于暂停状态;

		- 失败重试

			当一个客户端获取锁失败时，这个客户端应该在一个随机延时后进行重试。采用随机延时是为了避免不同客户端同时重试导致谁都无法拿到锁的情况出现，同样的道理客户端越快尝试在大多数Redis节点获取锁，出现多个客户端同时竞争锁和重试的时间窗口越小，可能性就越低，所以最完美的情况下，客户端应该用多路传输的方式同时向所有Redis节点发送SET命令。 这里非常有必要强调一下客户端如果没有在多数节点获取到锁，一定要尽快在获取锁成功的节点上释放锁，这样就没必要等到key超时后才能重新获取这个锁（但是如果网络分区的情况发生而且客户端无法连接到Redis节点时，会损失等待key超时这段时间的系统可用性）

		- 锁释放

			由于释放锁时会判断这个锁的value是不是自己设置的，如果是才删除；所以在释放锁时非常简单，只要向所有实例都发出释放锁的命令，不用考虑能否成功释放锁；

		- 锁过期

			1.每个实例key value ttl是一致的，但因为set的时间是不同的，所以出现每个实例过期的时间点是不一样的

- [redisson](https://www.cnblogs.com/wang-meng/p/12525029.html)

	- 加/解锁

		普通锁：RLock lock = redisson.getLock("anyLock");
		公平锁：RLock lock = redisson.getFairLock("anyLock");
		
		lock.lock();  
		lock.unlock();  
		
		红锁：  
		RLock lock1 = redisson1.getLock("lock1");  
		RLock lock2 = redisson2.getLock("lock2");  
		RLock lock3 = redisson3.getLock("lock3");  
		
		RLock redLock = anyRedisson.getRedLock(lock1, lock2, lock3);  
		
		// traditional lock method  
		redLock.lock();  
		
		// or acquire lock and automatically unlock it after 10 seconds  
		redLock.lock(10, TimeUnit.SECONDS);  
		
		// or wait for lock aquisition up to 100 seconds   
		// and automatically unlock it after 10 seconds  
		boolean res = redLock.tryLock(100, 10, TimeUnit.SECONDS);  
		if (res) {  
		   try {  
		     ...  
		   } finally {  
		       redLock.unlock();  
		   }  
		}  
		
		读写锁：  
		RReadWriteLock rwlock = redisson.getReadWriteLock("tryLock");  
		
		RLock lock = rwlock.readLock();  
		// or  
		RLock lock = rwlock.writeLock();  
		
		// traditional lock method  
		lock.lock();  
		
		// or acquire lock and automatically unlock it after 10 seconds  
		lock.lock(10, TimeUnit.SECONDS);  
		
		// or wait for lock aquisition up to 100 seconds   
		// and automatically unlock it after 10 seconds  
		boolean res = lock.tryLock(100, 10, TimeUnit.SECONDS);  
		if (res) {  
		   try {  
		     ...  
		   } finally {  
		       lock.unlock();  
		   }  
		}  
		
		Semaphore：
		RSemaphore semaphore = redisson.getSemaphore("semaphore");  
		// 同时最多允许3个线程获取锁
		semaphore.trySetPermits(3);  
		
		for(int i = 0; i < 10; i++) {  
		  new Thread(new Runnable() {  
		
		    @Override  
		    public void run() {  
		      try {  
		        System.out.println(new Date() + "：线程[" + Thread.currentThread().getName() + "]尝试获取Semaphore锁"); 
		        semaphore.acquire();  
		        System.out.println(new Date() + "：线程[" + Thread.currentThread().getName() + "]成功获取到了Semaphore锁，开始工作"); 
		        Thread.sleep(3000);    
		        semaphore.release();  
		        System.out.println(new Date() + "：线程[" + Thread.currentThread().getName() + "]释放Semaphore锁"); 
		      } catch (Exception e) {  
		        e.printStackTrace();  
		      }  
		    }  
		  }).start();  
		}  
		
		CountDownLatch：
		RCountDownLatch latch = redisson.getCountDownLatch("anyCountDownLatch");  
		latch.trySetCount(3);  
		System.out.println(new Date() + "：线程[" + Thread.currentThread().getName() + "]设置了必须有3个线程执行countDown，进入等待中。。。"); 
		
		for(int i = 0; i < 3; i++) {  
		  new Thread(new Runnable() {  
		
		    @Override  
		    public void run() {  
		      try {  
		        System.out.println(new Date() + "：线程[" + Thread.currentThread().getName() + "]在做一些操作，请耐心等待。。。。。。"); 
		        Thread.sleep(3000);   
		        RCountDownLatch localLatch = redisson.getCountDownLatch("anyCountDownLatch");  
		        localLatch.countDown();  
		        System.out.println(new Date() + "：线程[" + Thread.currentThread().getName() + "]执行countDown操作"); 
		      } catch (Exception e) {  
		        e.printStackTrace();   
		      }  
		    }  
		
		  }).start();  
		}  
		
		latch.await();  
		System.out.println(new Date() + "：线程[" + Thread.currentThread().getName() + "]收到通知，有3个线程都执行了countDown操作，可以继续往下走"); 

		- watchDog原理

			如果一个场景：现在有A，B在执行业务，A加了分布式锁，但是生产环境是各种变化的，如果万一A锁超时了，但是A的业务还在跑。而这时由于A锁超时释放，B拿到锁，B执行业务逻辑。这样分布式锁就失去了意义？
			
			所以Redisson 引入了watch dog的概念，当A获取到锁执行后，如果锁没过期，有个后台线程会自动延长锁的过期时间，防止因为业务没有执行完而锁过期的情况。
			
			我们接着来看看具体实现：  
			private <T> RFuture<Long> tryAcquireAsync(long leaseTime, TimeUnit unit, final long threadId) {  
			    if (leaseTime != -1) {  
			        return tryLockInnerAsync(leaseTime, unit, threadId, RedisCommands.EVAL_LONG);  
			    }  
			    RFuture<Long> ttlRemainingFuture = tryLockInnerAsync(commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG);  
			    ttlRemainingFuture.addListener(new FutureListener<Long>() {  
			        @Override  
			        public void operationComplete(Future<Long> future) throws Exception {  
			            if (!future.isSuccess()) {  
			                return;  
			            }  
			
			            Long ttlRemaining = future.getNow();  
			            // lock acquired  
			            if (ttlRemaining == null) {  
			                scheduleExpirationRenewal(threadId);  
			            }  
			        }  
			    });  
			    return ttlRemainingFuture;  
			}  
			
			当我们tryLockInnerAsync执行完之后，会添加一个监听器，看看监听器中的具体实现：
			
			
			protected RFuture<Boolean> renewExpirationAsync(long threadId) {  
			    return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN,  
			            "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " +  
			                "redis.call('pexpire', KEYS[1], ARGV[1]); " +  
			                "return 1; " +  
			            "end; " +  
			            "return 0;",  
			        Collections.<Object>singletonList(getName()),   
			        internalLockLeaseTime, getLockName(threadId));  
			}  
			
			这里面调度任务每隔10s钟执行一次，lua脚本中是续约过期时间，使得当前线程持有的锁不会因为过期时间到了而失效

### [基于Zookeeper](https://www.cnblogs.com/wang-meng/p/12593992.html)

- 基于临时节点

	ZooKeeper机制规定：同一个目录下只能有一个唯一的文件名。例如：我们在Zookeeper目录/test目录下创建，两个客户端创建一个名为Lock节点，只有一个能够成功。即只有一个服务能创建成功。即创建成功的服务获取分布式锁，解锁时 ，删除相应的节点即可，其余客户端再次进入竞争创建节点，直到所有的客户端都能获得锁。完成自己的业务逻辑

- 基于顺序临时节点

	假如我们在/lock/目录下创建节3个点，ZooKeeper集群会按照提起创建的顺序来创建节点，节点分别为/lock/0001、/lock/0002、/lock/0003  
	
	ZK的临时节点特性  当客户端与ZK集群断开连接，则临时节点自动被删除  
	
	客户端调用create()方法创建名为“test/lock-*”的节点，需要注意的是，这里节点的创建类型需要设置为EPHEMERAL_SEQUENTIAL。  
	
	客户端调用getChildren(“lock-*”)方法来获取所有已经创建的子节点，同时在这个节点上注册上子节点变更通知的Watcher。  
	
	客户端获取到所有子节点path之后，循环子节点发现创建的节点是所有节点中序号最小的，那么就认为这个客户端获得了锁。如果发现循环过程中发现自己创建的节点不是最小的，说明自己还没有获取到锁，就开始等待，直到下次子节点变更通知的时候，再进行子节点的获取，判断是否获取锁。

- 集群版问题

- Curator客户端

	- 加/解锁

		private static final String ZK_ADDRESS = "192.20.38.58:2181";  
		private static final String ZK_LOCK_PATH = "/locks/lock_01";  
		CuratorFramework client = CuratorFrameworkFactory.newClient(ZK_ADDRESS, new RetryNTimes(10, 5000));  
		
		普通锁：  
		InterProcessMutex lock = new InterProcessMutex(client, ZK_LOCK_PATH);  
		lock.acquire(10 * 1000, TimeUnit.SECONDS)  
		
		
		Semaphore锁：  
		InterProcessSemaphoreV2 semaphore = new InterProcessSemaphoreV2(client, ZK_SEMAPHORE_LOCK_PATH, 2);  
		Lease lease = semaphore.acquire();

- curator框架

	- 共享锁  InterProcessMutex

	- 读写锁  InterProcessReadWriteLock

	- 共享信号量 InterProcessSemaphoreV2

- 可能出现的问题

	由于网络抖动，ZK和客户端连接中断，临时结点被删除，于是其他客户端获取到锁，违背锁的排他性。幸好ZK有重试机制，一定程度避免该情况发生

## 分布式集群管理

### Zookeeper

最终一致性：保证各个节点服务器数据能够最终达成一致，zk的招牌功能  

顺序性：从同一客户端发起的事务请求，都会最终被严格的按照其发送顺序被应用到zk中，这也是zk选举leader的依据之一  

可靠性：凡是服务器成功的使用一个事务，并完成了客户端的响应，那么这个事务所引起的服务端状态变更会被一直保留下去  

实时性：zk不能保证多个客户端能同时得到刚更新的数据，所以如果要最新数据，需要在读数据之前强制调用sync接口来保证数据的实时性  

原子性：数据更新要么成功要么失败  
                                                        
单一视图：无论客户端连的是哪个节点，看到的数据模型对外一致

- api

	create：创建节点
	
	delete：删除节点
	
	exists：判断节点是否存在
	
	getData：获得一个节点的数据
	
	setData：设置一个节点的数据
	
	getChildren：获取节点下的所有子节点

	- watch事件机制

		exists，getData，getChildren属于读操作。Zookeeper客户端在请求读操作的时候，可以选择是否设置Watch。Watch是注册在特定Znode上的触发器。当这个Znode发生改变，也就是调用了create，delete，setData方法的时候，将会触发Znode上注册的对应事件，请求Watch的客户端会接收到异步通知
		
		1.客户端调用getData方法，watch参数是true。服务端接到请求，返回节点数据，并且在对应的哈希表里插入被Watch的Znode路径，以及Watcher列表。
		
		2.当被Watch的Znode已删除，服务端会查找哈希表，找到该Znode对应的所有Watcher，异步通知客户端，并且删除哈希表中对应的Key-Value。

		- DataWatches

		- Child Watches

			基于znode的孩子节点发生变更触发的watch事件，触发条件 getChildren()、 create()基于znode节点的数据变更从而触发 watch 事件，触发条件getData()、exists()、setData()、 create()

- ZNode节点

	data：Znode存储的数据信息
	
	ACL：记录Znode的访问权限，即哪些人或哪些IP可以访问本节点
	
	stat：包含Znode的各种元数据，比如事务ID、版本号、时间戳、大小等等。
	
	child：当前节点的子节点引用，类似于二叉树的左孩子右孩子
	
	Zxid：节点状态改变会导致该节点收到一个zxid格式的时间戳  
		cZxid（节点创建时间）  
		mZxid（该节点修改时间，与子节点无关）  
		pZxid（该节点或者该节点的子节点的最后一次创建或者修改时间，孙子节点无关）  
	
	适合读多写少环境，每个节点的数据最大不能超过1MB

	- ACL

		基于ACL实现znode的访问权限控制，作用于znode级别，不可继承
		
		格式：<schema>:<id>:<acl>  
		
		schema：授权方式  
			world：任何人都可以访问  
			auth：只有认证的用户可以访问  
			digest：使用username ：password用户密码生成MD5哈希值作为认证ID  
			host/ip：使用客户端主机IP地址来进行认证  
		
		id： 权限的作用域，用来标识身份，依赖于schema选择哪种方式  
		
		acl：给一个节点赋予哪些权限，节点的权限有create,、delete、write、read、admin 统称 cdwra

	- Version

		dataversion（数据版本号）
		cversion（子节点版本号）
		aclversion（节点所拥有的ACL权限版本号）
		
		对znode节点数据修改后，该节点的dataversion版本号会递增

- 日志

	- 数据快照

- 三个端口

	1、2181：对clint端提供服务
	
	2、3888：选举leader使用
	
	3、2888：集群内机器通讯使用（Leader监听此端口）

- zoo.cfg详解

	1.tickTime：心跳间隔时间，默认2000ms，FL会话时长通常是心跳时间两倍
	2.initLimit：FL之间初始连接时能容忍的最多心跳周期  
	3.syncLimit：FL之间请求和答应最多能容忍的心跳数  
	4.dataDir：存放myid信息跟一些版本，日志，跟服务器唯一的ID信息等  
	5.clientPort：2181端口提供客户端和服务器通讯

- 集群

	- 节点

		- 两个属性(集群)

			- sid

				服务器ID，部署的时候指定

			- zxid

				高32位表示该事务发生的集群选举周期epoch（集群每发生一次leader选举，值加1），低32位表示该事务在当前选择周期内的递增次序（leader每处理一个事务请求，值加1，发生一次leader选择，低32位要清0）

		- 三种角色(集群)

			- Leader

				既可以为客户端提供写服务又能提供读服务

			- Follower

				返回读请求，转发写请求给leader，参与选举投票

			- Observer

				返回读请求，转发写请求给leader，不参与选举投票

		- 四个状态(集群)

			- LEADING

				当前Server即为选举出来的Leader

			- LOOKING

				当前Server不知道Leader是谁，正在搜寻

			- OBSERVING

				表明当前服务器角色是Observer

			- FOLLOWING

				表明当前服务器角色是Follower

		- 四种类型(集群)

			- PERSISTENT

				持久化目录节点：客户端与ZK断开连接后，该节点依旧存在 

			- EPHEMERAL

				临时目录节点：客户端与ZK断开连接后，该节点被删除

			- PERSISTENT_SEQUENTIAL

				持久化顺序编号目录节点：客户端与ZK断开连接后，该节点依旧存在，只是ZK给该节点名称进行顺序编号

			- EPHEMERAL_SEQUENTIAL

				临时顺序编号目录节点：客户端与ZK断开连接后，该节点被删除，只是ZK给该节点名称进行顺序编号

	- 基于ZAB协
	  议的一致性

		- 选举制度

			- 投票过程

				选票数据结构：（logicClock，vote_id，vote_zxid）  
				logicClock 表示这是该服务器发起的第多少轮投票，从1开始计数  
				state 当前服务器的状态  
				self_id 当前服务器的唯一ID  
				self_zxid 当前服务器上所保存的数据的最大事务ID，从0开始计数  
				vote_id 被推举的服务器的唯一ID  
				vote_zxid 被推举的服务器上所保存的数据的最大事务ID，从0开始计数  
				
				收到投票：  
				1.zxid相等：serverid大的胜出  
				2.zxid小于：说明本地投票过时了，外部投票胜出，外部投票信息覆盖本地投票，并帮忙广播该投票信息  
				3.zxid大于：忽略该投票  
				4.每收集到一个投票后，查看已经收到的投票结果记录列表，看是否有节点能够达到一半以上的投票数。如果有达到，则终止投票，宣布选举结束，更新自身状态。然后进行发现和同步阶段

				- 详细过程

						1. 自增选举轮次。Zookeeper规定所有有效的投票都必须在同一轮次中，在开始新一轮投票时，会首先对logicalclock进行自增操作。
					
					　　2. 初始化选票。在开始进行新一轮投票之前，每个服务器都会初始化自身的选票，并且在初始化阶段，每台服务器都会将自己推举为Leader。
					
					　　3. 发送初始化选票。完成选票的初始化后，服务器就会发起第一次投票。Zookeeper会将刚刚初始化好的选票放入sendqueue中，由发送器WorkerSender负责发送出去。
					
					　　4. 接收外部投票。每台服务器会不断地从recvqueue队列中获取外部选票。如果服务器发现无法获取到任何外部投票，那么就会立即确认自己是否和集群中其他服务器保持着有效的连接，如果没有连接，则马上建立连接，如果已经建立了连接，则再次发送自己当前的内部投票。
					
					　　5. 判断选举轮次。在发送完初始化选票之后，接着开始处理外部投票。在处理外部投票时，会根据选举轮次来进行不同的处理。
					
					　　　　· 外部投票的选举轮次大于内部投票。若服务器自身的选举轮次落后于该外部投票对应服务器的选举轮次，那么就会立即更新自己的选举轮次(logicalclock)，并且清空所有已经收到的投票，然后使用初始化的投票来进行PK以确定是否变更内部投票。最终再将内部投票发送出去。
					
					　　　　· 外部投票的选举轮次小于内部投票。若服务器接收的外选票的选举轮次落后于自身的选举轮次，那么Zookeeper就会直接忽略该外部投票，不做任何处理，并返回步骤4。
					
					　　　　· 外部投票的选举轮次等于内部投票。此时可以开始进行选票PK。
					
					　　6. 选票PK。在进行选票PK时，符合任意一个条件就需要变更投票。
					
					　　　　· 若外部投票中推举的Leader服务器的选举轮次大于内部投票，那么需要变更投票。
					
					　　　　· 若选举轮次一致，那么就对比两者的ZXID，若外部投票的ZXID大，那么需要变更投票。
					
					　　　　· 若两者的ZXID一致，那么就对比两者的SID，若外部投票的SID大，那么就需要变更投票。
					
					　　7. 变更投票。经过PK后，若确定了外部投票优于内部投票，那么就变更投票，即使用外部投票的选票信息来覆盖内部投票，变更完成后，再次将这个变更后的内部投票发送出去。
					
					　　8. 选票归档。无论是否变更了投票，都会将刚刚收到的那份外部投票放入选票集合recvset中进行归档。recvset用于记录当前服务器在本轮次的Leader选举中收到的所有外部投票（按照服务队的SID区别，如{(1, vote1), (2, vote2)…}）。
					
					　　9. 统计投票。完成选票归档后，就可以开始统计投票，统计投票是为了统计集群中是否已经有过半的服务器认可了当前的内部投票，如果确定已经有过半服务器认可了该投票，则终止投票。否则返回步骤4。
					
					　　10. 更新服务器状态。若已经确定可以终止投票，那么就开始更新服务器状态，服务器首选判断当前被过半服务器认可的投票所对应的Leader服务器是否是自己，若是自己，则将自己的服务器状态更新为LEADING，若不是，则根据具体情况来确定自己是FOLLOWING或是OBSERVING。
					
					　　以上10个步骤就是FastLeaderElection的核心，其中步骤4-9会经过几轮循环，直到有Leader选举产生。

			- 启动选主

				1.同时启动：认为事务zxid一致，比较sid
				
				2.非同时启动：启动过程如果某个节点完成选主，后续服务器sid大于该服务器也无济于事  
				
				3.宕机启动：依照投票规则选主

			- QuorumCnxManager

				- 消息队列

					内部维护了一系列的队列，用来保存接收到的、待发送的消息以及消息的发送器，除接收队列以外，其他队列都按照SID分组形成队列集合，如一个集群中除了自身还有3台机器，那么就会为这3台机器分别创建一个发送队列，互不干扰
					
					
					· recvQueue：消息接收队列，用于存放那些从其他服务器接收到的消息。  
					
					· queueSendMap：消息发送队列，用于保存那些待发送的消息，按照SID进行分组。  
					
					· senderWorkerMap：发送器集合，每个SenderWorker消息发送器，都对应一台远程Zookeeper服务器，负责消息的发送，也按照SID进行分组。  
					
					· lastMessageSent：最近发送过的消息，为每个SID保留最近发送过的一个消息。

				- 建立连接

					启动时会创建一个ServerSocket来监听Leader选举的通信端口(默认为3888)。开启监听后，Zookeeper能够不断地接收到来自其他服务器的创建连接请求，在接收到其他服务器的TCP连接请求时，会进行处理。为了避免两台机器之间重复地创建TCP连接，Zookeeper只允许SID大的服务器主动和其他机器建立连接，否则断开连接。在接收到创建连接请求后，服务器通过对比自己和远程服务器的SID值来判断是否接收连接请求，如果当前服务器发现自己的SID更大，那么会断开当前连接，然后自己主动和远程服务器建立连接。一旦连接建立，就会根据远程服务器的SID来创建相应的消息发送器SendWorker和消息接收器RecvWorker，并启动

				- 消息接收

					ZK为每个远程服务器都分配一个单独的RecvWorker，因此，每个RecvWorker只需要不断地从这个TCP连接中读取消息，并将其保存到recvQueue队列中

				- 消息发送

					ZK为每个远程服务器都分配一个单独的SendWorker，因此，每个SendWorker只需要不断地从对应的消息发送队列中获取出一个消息发送即可，同时将这个消息放入lastMessageSent中

		- 恢复模式(选主/同步)

			1.Leader崩溃即进入恢复模式
			
			2.设置状态为LOOKING，初始化内部投票Vote (id,zxid) 数据至内存，并将其广播到集群其它节点  
			
			3.恢复之后得同步，当Follower连接上Leader之后，Leader服务器会根据自己服务器上最后被提交的ZXID和Follower上的ZXID进行比对，比对结果要么回滚，要么和Leader同步  
			
			4.这时候当一个server加入zookeeper服务中，它会在恢复模式下启动，发现leader，并和leader进行状态同步。待到同步结束，它也参与消息广播

		- 消息广播(过半写)

			消息广播基于原子广播协议，类似二阶段提交过程：  
			1.客户端请求Leader或Follower转发写请求给Leader，Leader为请求封装全局递增事务ID(zxid)；  
			2.Leader写入自己日志系统，然后依靠消息队列将数据复制到Follower；  
			3.Follwer首先将其以事务日志的形式写入本地磁盘中，然后回应Ack，最低超过半数本次事务即可成功；  
			4.当超过半数成功回应，则执行commit，同时提交自己，并且发送commit到Follower

- 应用

	- 分布式锁

	- 分布式事务

	- 注册中心

		- 服务提供者

			在初始化启动时在特定节点创建子节点，同时写入url地址

		- 服务消费者

			在初始化启动时获取特定节点所有子节点，同时将自己的url地址注册到特定节点

		- 监控中心

			监控中心在启动的时候会通过ZK的特定节点来获取所有提供者和消费者的url地址，并注册Watcher来监听其子节点变化情况

- 面试题

	- 脑裂问题

		明确一点：得于过半机制ZK集群不会出现脑裂问题
		
		假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用

	- Commit数据不丢失

		确保已经被leader提交的proposal必须最终被所有的follower服务器提交
		
		1.只要leader提出的proposal被过半的节点接收到，那么就算是被leader提交成功了(不论leader有没发出commit命令)，如果leader根本没发出commit命令就挂了，那么后续被选举的新leader（zxid最大的那个）,会首先判断自身未Commit的消息是否存在于大多数服务器中从而决定是否要将其Commit
		
		2.新的 leader 将自己事务日志中 proposal 但未 COMMIT 的消息处理
		
		3.新的 leader 与 follower 建立先进先出的队列， 先将自身有而 follower 没有的 proposal 发送给 follower，再将这些 proposal 的 COMMIT 命令发送给 follower，以保证所有的 follower 都保存了所有的 proposal、所有的 follower 都处理了所有的消息。通过以上策略，能保证已经被处理的消息不会丢

	- 丢弃未Commit数据

		Zab 通过巧妙的设计 zxid 来实现这一目的。一个 zxid 是64位，高 32 是纪元（epoch）编号，每经过一次 leader 选举产生一个新的 leader，新 leader 会将 epoch 号 +1。低 32 位是消息计数器，每接收到一条消息这个值 +1，新 leader 选举后这个值重置为 0。这样设计的好处是旧的 leader 挂了后重启，它不会被选举为 leader，因为此时它的 zxid 肯定小于当前的新 leader。当旧的 leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的 proposal 清除

	- 动态添加机器

		3.5版本开始支持动态扩容  
		全部重启：关闭所有Zookeeper服务，修改配置之后启动。不影响之前客户端的会话
		逐个重启：在过半存活即可用的原则下，一台机器重启不影响整个集群对外提供服务。这是比较常用的方式

## 分布式缓存

### Redis

redis是通过socket访问到缓存服务，效率比ecache低

- 高可用

	1.持久化
	
	2.复制
	
	3.哨兵
	
	4.集群
	
	四步走  
	
	Redis 哨兵 着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务  
	
	Redis Cluster 着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储

	- [主从](https://www.cnblogs.com/kismetv/p/9236731.html)

		- 过程

			- 1.连接建立阶段

				1.(从节点)slaveof <masterip> <masterport>建立主从复制关系
				
				2.(从节点)slaveof no one断开  
				
				3.从节点主动跟主节点进行身份验证，建立socket连接

			- 2.数据同步阶段

				该阶段主从节点互为客户端，主向从发送请求（如推送缓冲区中的写命令），完成数据同步

				- 全量复制

					用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点  
					
					1）从服务器连接主服务器，发送SYNC命令  
					2）主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令  
					3）主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令  
					4）从服务器收到快照文件后丢弃所有旧数据，载入收到的快照  
					5）主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令  
					6）从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令

					- SYNC命令

				- 命令传播

					Master节点每处理完一个命令都会把命令广播给所有的子节点

				- 部分复制

					用于网络中断等情况后的复制，只将中断期间主节点执行的写命令发送给从节点

					- 复制偏移量

						双方各自维护一个偏移量，Master成功发送N个字节的命令后会将Master的offset加上N，Slave在接收到N个字节命令后同样会将Slave的offset增加N。Master和Slave如果状态是一致的那么它的的offset也应该是一致的

					- 复制积压缓冲区

						由主节点维护，备份主节点最近发送给从节点的数据

					- 服务器运行ID(runid)

						每个Redis节点(无论主从)，在启动时都会自动生成一个随机ID(每次启动都不一样)
						
						主要作用是判断是新的从节点还是曾经连接过又断开的节点，好决定全量复制还是部分复制

					- PSYNC命令

						PSYNC <runid> <offset>  
						runid:主服务器ID
						offset:从服务器最后接收命令的偏移量
						  PSYNC执行过程中比较重要的概念有3个：runid、offset（复制偏移量）以及复制积压缓冲区。

				- 复制策略

					主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步

			- 3.命令传播阶段

				于异步方式，主节点将自己执行的每一个写命令发送给从节点，从节点接收命令并执行，从而保证主从节点数据的一致性

				- 心跳机制

					每隔指定的时间（默认值是10s），主节点会向从节点发送PING命令

				- REPLCONF ACK

					从节点向主节点发送REPLCONF ACK命令，频率是每秒1次；命令格式为：REPLCONF ACK {offset}，其中offset指从节点保存的复制偏移量
					
					1.实时监测主从节点网络状态  
					
					2.检测命令丢失  
					
					3.辅助保证从节点的数量和延迟

				- 延迟与不一致问题

					主节点发送写命令后并不会等待从节点的回复

		- 问题

			- 数据过期问题

				主从模式下，从不主动删除过期key，幸运的是3.2版本允许从自主判断过期key

			- 故障切换问题

			- 复制超时问题

			- 复制中断问题

	- [哨兵](https://juejin.im/post/5b7d226a6fb9a01a1e01ff64)

		- 哨兵节点

			- 监控（Monitoring）

				Sentinel会不断地检查你的主服务器和从服务器是否运作正常

			- 提醒（Notification）

				当被监控的某个Redis服务器出现问题时，Sentinel可以通过API向管理员或者其他应用程序发送通知

			- 自动故障迁移（Automatic failover）

				当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会进行选举，将其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器

			- 基本原理

				- 3个定时任务

					每个哨兵节点维护3个定时任务：
					
					1.每10秒执行info获取最新主从结构  
					
					2.每2秒主节点的channel交换信息(pub/sub)获取其他哨兵节点的信息  
					
					3.每1秒对其他哨兵和主从节点进行ping判断是否下线

				- 主观下线

					在心跳检测的定时任务中，如果其他节点超过一定时间没有回复，哨兵节点就会将其进行主观下线。主观下线的意思是一个哨兵节点“主观地”判断下线

				- 客观下线

					哨兵节点在对主节点进行主观下线后，会通过sentinel is-master-down-by-addr命令询问其他哨兵节点该主节点的状态；如果判断主节点下线的哨兵数量达到一定数值，则对该主节点进行客观下线
					
					客观下线是主节点才有的概念；如果从节点和哨兵节点发生故障，被哨兵主观下线后，不会再有后续的客观下线和故障转移操作

					- 询问其他哨兵

						SENTINEL is-master-down-by-addr <ip><port><current_epoch><runid>命令询问其它Sentinel是否同意Master已下线
						
						ip：主节点IP地址
						port：主节点端口
						current_epoch：Sentinel当前的配置纪元，用于选举领头Sentinel
						runid：分为Sentinel的运行id和“.”。运行id用于选举领头Sentinel，“.”代表检测主服务的客观下线状态

						- 主动询问的哨兵渴望成为哨兵领导者

					- 其他哨兵返回信息

						返回包含三个信息的回复：  
						1)<down_state>  
						
						2)<leader_runid>  
						
						3)<leader_epoch>  
						
						
						down_state：返回对主节点的检查结果，1代表主节点已下线，0代表主节点未下线  
						leader_runid：哨兵运行ID用于选举领头Sentinel，“.”用于检测主节点的下线状态  
						leader_epoch：目标哨兵的局部领头哨兵的配置纪元，用于选举领头哨兵

					- 分析其他哨兵返回

						基于过半机制，判断主节点需要下线后，将主节点实例结构glags属性的SRI_O_DOWN表示打开，表示主节点已经进入客观下线状态

				- 选举领导者哨兵节点

					选举出一个领导者哨兵节点，并由该领导者节点对主节点进行故障转移操作，具体算法思想👈

					- 选举算法Raft算法

						1.每个做主观下线的sentinel节点向其他sentinel节点发送命令，要求将自己设置为领导者  
						
						2、接收到的sentinel可以同意或者拒绝  
						
						3、如果该sentinel节点发现自己的票数已经超过半数并且超过了quorum  
						
						4、如果此过程选举出了多个领导者，那么将等待一段时重新进行选举

					- 故障转移

						领导者哨兵，开始进行故障转移操作

						- 于从点内选新主点

							首先过滤掉不健康的从节点，然后  
							1.选择优先级最高的从节点(由slave-priority指定)
							2.如果优先级无法区分，则选择复制偏移量最大的从节点  
							3.如果仍无法区分，则选择runid最小的从节点

						- 更新主从状态

							通过slaveof no one命令，让选出来的从节点成为主节点；并通过slaveof命令让其他节点成为其从节点

						- 更新下线节点状态

							将已经下线的主节点(即6379)设置为新的主节点的从节点，当6379重新上线后，它会成为新的主节点的从节点

			- 客户端高可用

				1.首先客户端是不知道故障转移的发生
				
				2.客户端在维护哨兵池子，通过哨兵节点获取主从节点IP地址等信息  
				
				3.客户端只有在初始化和切换主节点时需要和Sentinel节点集合进行交互来获取主节点信息

		- 基本命令

			info sentinel：获取监控的所有主节点的基本信息
			sentinel masters：获取监控的所有主节点的详细信息
			sentinel master mymaster：获取监控的主节点mymaster的详细信息
			sentinel slaves mymaster：获取监控的主节点mymaster的从节点的详细信息
			sentinel sentinels mymaster：获取监控的主节点mymaster的哨兵节点的详细信息
			sentinel get-master-addr-by-name mymaster：获取监控的主节点mymaster的地址信息
			sentinel is-master-down-by-addr：哨兵节点之间可以通过该命令询问主节点是否下线，从而对是否客观下线做出判断
			sentinel failover mymaster：强制对mymaster执行故障转移

	- [集群-cluster](https://www.cnblogs.com/kismetv/p/9853040.html)

		- 搭建过程

			- 启动节点

				将节点以集群模式启动，此时节点是独立的，并没有建立联系  
				
				# redis-7000.conf  
				port 7000  
				# 启动集群模式  
				cluster-enabled yes  
				# 集群配置文件  
				cluster-config-file "node-7000.conf"  
				logfile "log-7000.log"  
				dbfilename "dump-7000.rdb"  
				daemonize yes  
				
				启动命令：redis-server redis-7000.conf  
				
				集群配置文件：每个节点在运行过程中，会维护一份集群配置文件；每当集群架构，集群内所有节点会将最新信息更新到该配置文件；当节点重启后，会重新读取该配置文件，获取集群信息，可以方便的重新加入到集群中。如果没有，则初始化配置并将配置保存到文件中。集群配置文件由Redis节点维护，不需要人工修改

			- 节点握手

				让独立的节点连成一个网络  
				
				节点握手通过cluster meet {ip} {port}命令实现，例如在7000节点中执行cluster meet 192.168.72.128 7001，可以完成7000节点和7001节点的握手。通过节点之间的通信，每个节点都可以感知到所有其他节点

				- A cluster meet B

					1)  A为B创建一个clusterNode结构，并将其添加到clusterState的nodes字典中
					
					2)  A向B发送MEET消息
					
					3)  B收到MEET消息后，会为A创建一个clusterNode结构，并将其添加到clusterState的nodes字典中
					
					4)  B回复A一个PONG消息
					
					5)  A收到B的PONG消息后，便知道B已经成功接收自己的MEET消息
					
					6)  然后，A向B返回一个PING消息
					
					7)  B收到A的PING消息后，便知道A已经成功接收自己的PONG消息，握手完成
					
					8)  之后，A通过Gossip协议将B的信息广播给集群内其他节点，其他节点也会与B握手；一段时间后，集群收敛，B成为集群内的一个普通节点
					
					通过上述过程可以发现，集群中两个节点的握手过程与TCP类似，都是三次握手：
					A向B发送MEET；
					B向A发送PONG；
					A向B发送PING。

					- 三次握手！

			- 分配槽

				将16384个槽分配给主节点
				
				
				分配槽使用cluster addslots命令，执行下面的命令将槽（编号0-16383）全部分配完毕：
				
				redis-cli -p 7000 cluster addslots {0..5461}  
				redis-cli -p 7001 cluster addslots {5462..10922}  
				redis-cli -p 7002 cluster addslots {10923..16383}

				- cluster addslots

			- 指定主从关系

				为从节点指定主节点  
				
				通过cluster nodes获得几个主节点的节点id后，执行下面的命令为每个从节点指定主节点：  
				
				redis-cli -p 8000 cluster replicate [nodeId]

		- 作用

			- 数据分片

				- 客户端访问集群

					- redis-cli

						1.(cluster keyslot key)计算key属于哪个槽：CRC16(key) & 16383
						
						2.判断key所在的槽是否在当前节点：假设key位于第i个槽，clusterState.slots[i]则指向了槽所在的节点，如果clusterState.slots[i]==clusterState.myself，说明槽在当前节点，可以直接在当前节点执行命令；否则，说明槽不在当前节点，则查询槽所在节点的地址(clusterState.slots[i].ip/port)，并将其包装到MOVED错误中返回给redis-cli
						
						3.redis-cli收到MOVED错误后，根据返回的ip和port重新发送请求
						
						4.如果正在移槽，客户端会被引导去目标节点寻找

					- JedisCluster

						1.JedisCluster初始化时，在内部维护slot->node的缓存，方法是连接任一节点，执行cluster slots命令
						
						2.JedisCluster为每个节点创建连接池(即JedisPool)
						
						3.当执行命令时，JedisCluster根据key->slot->node选择需要连接的节点，发送命令。如果成功，则命令执行完毕。如果执行失败，则会随机选择其他节点进行重试，并在出现MOVED错误时，使用cluster slots重新同步slot->node的映射关系

				- 数据分片算法

					常用的数据分片的方法有：范围分片，哈希分片，一致性哈希算法，哈希槽等  
					
					Redis采用虚拟哈希槽算法：  
					
					根据CRC16算法，计算key归属的节点

			- 高可用

				集群支持主从复制和主节点的自动故障转移（与哨兵类似）；当任一节点发生故障时，集群仍然可以对外提供服务

		- 节点通信机制

			- 两个端口

				- 普通端口

					即我们在前面指定的端口(7000等)。普通端口主要用于为客户端提供服务（与单机节点类似）；但在节点间数据迁移时也会使用

				- 集群端口

					集群中的每个节点都会单独开辟一个TCP通道，用于节点之间彼此通信，通信端口号在基础端口上加10000

			- Gossip协议

				广播是指向集群内所有节点发送消息；优点是集群的收敛速度快(集群收敛是指集群内所有节点获得的集群信息是一致的)，缺点是每条消息都要发送给所有节点，CPU、带宽等消耗较大。
				
				Gossip协议的特点是：在节点数量有限的网络中，每个节点都“随机”的与部分节点通信（并不是真正的随机，而是根据特定的规则选择通信的节点），经过一番杂乱无章的通信，每个节点的状态很快会达到一致。Gossip协议的优点有负载(比广播)低、去中心化、容错性高(因为通信有冗余)等；缺点主要是集群的收敛速度慢 

				- meet消息

					在节点握手阶段，当节点收到客户端的CLUSTER MEET命令时，会向新加入的节点发送MEET消息，请求新节点加入到当前集群；新节点收到MEET消息后会回复一个PONG消息

				- ping消息

					集群里每个节点每秒钟会选择部分节点发送PING消息，接收者收到消息后会回复一个PONG消息。PING消息的内容是自身节点和部分其他节点的状态信息；作用是彼此交换信息，以及检测节点是否在线。PING消息使用Gossip协议发送，接收节点的选择兼顾了收敛速度和带宽成本，具体规则如下：(1)随机找5个节点，在其中选择最久没有通信的1个节点(2)扫描节点列表，选择最近一次收到PONG消息时间大于cluster_node_timeout/2的所有节点，防止这些节点长时间未更新

				- pong消息

					PONG消息封装了自身状态数据。可以分为两种：第一种是在接到MEET/PING消息后回复的PONG消息；第二种是指节点向集群广播PONG消息，这样其他节点可以获知该节点的最新信息，例如故障恢复后新的主节点会广播PONG消息

				- fail消息

					当一个主节点判断另一个主节点进入FAIL状态时，会向集群广播这一FAIL消息；接收节点会将这一FAIL消息保存起来，便于后续的判断

				- publish消息

					节点收到PUBLISH命令后，会先执行该命令，然后向集群广播这一消息，接收节点也会执行该PUBLISH命令

		- 数据结构

			节点需要专门的数据结构来存储集群的状态。包括：集群是否处于上线状态、集群中有哪些节点、节点是否可达、节点的主从状态、槽的分布……

			- clusterNode

				录了一个节点的状态，每个节点都会用一个clusterNode结构记录自己的状态，并为集群内所有其他节点都创建一个clusterNode结构来记录节点状态  
				
				typedef struct clusterNode {  
				    //节点创建时间  
				    mstime_t ctime;  
				   
				    //节点id  
				    char name[REDIS_CLUSTER_NAMELEN];  
				   
				    //节点的ip和端口号  
				    char ip[REDIS_IP_STR_LEN];  
				    int port;  
				   
				    //节点标识：整型，每个bit都代表了不同状态，如节点的主从状态、是否在线、是否在握手等  
				    int flags;  
				   
				    //配置纪元：故障转移时起作用，类似于哨兵的配置纪元  
				    uint64_t configEpoch;  
				   
				    //槽在该节点中的分布：占用16384/8个字节，16384个比特；每个比特对应一个槽：比特值为1，则该比特对应的槽在节点中；比特值为0，则该比特对应的槽不在节点中  
				    unsigned char slots[16384/8];  
				   
				    //节点中槽的数量  
				    int numslots;  
				   
				    …………  
				   
				} clusterNode;

			- clusterState

				保存了在当前节点视角下，集群所处的状态  
				
				typedef struct clusterState {  
				   
				    //自身节点  
				    clusterNode *myself;  
				   
				    //配置纪元  
				    uint64_t currentEpoch;  
				   
				    //集群状态：在线还是下线  
				    int state;  
				   
				    //集群中至少包含一个槽的节点数量  
				    int size;  
				   
				    //哈希表，节点名称->clusterNode节点指针  
				    dict *nodes;  
				    
				    //槽分布信息：数组的每个元素都是一个指向clusterNode结构的指针；如果槽还没有分配给任何节点，则为NULL  
				    clusterNode *slots[16384];  
				   
				    …………  
				       
				} clusterState;

		- 实践须知

			- 集群伸缩

				伸缩的核心是槽迁移：修改槽与节点的对应关系，实现槽(即数据)在节点之间的移动
				
				例如，如果槽均匀分布在集群的3个节点中，此时增加一个节点，则需要从3个节点中分别拿出一部分槽给新节点，从而实现槽在4个节点中的均匀分布。

				- ASK错误

					槽迁移时，客户端向源节点发送命令，客户端收到ASK错误后，从中读取目标节点的地址信息，并向目标节点重新发送请求

			- 故障转移

				集群的实现与哨兵思路类似：通过定时任务发送PING消息检测其他节点状态；节点下线分为主观下线和客观下线；客观下线后选取从节点进行故障转移。
				
				与哨兵一样，集群只实现了主节点的故障转移；从节点故障时只会被下线，不会进行故障转移。因此，使用集群时，应谨慎使用读写分离技术，因为从节点故障会导致读服务不可用，可用性变差。  
				
				节点数量：在故障转移阶段，需要由主节点投票选出哪个从节点成为新的主节点；从节点选举胜出需要的票数为N/2+1；其中N为主节点数量(包括故障主节点)，但故障主节点实际上不能投票。因此为了能够在故障发生时顺利选出从节点，集群中至少需要3个主节点(且部署在不同的物理机上)。
				
				故障转移时间：从主节点故障发生到完成转移，所需要的时间主要消耗在主观下线识别、主观下线传播、选举延迟等几个环节；具体时间与参数cluster-node-timeout有关，一般来说：
				
				故障转移时间(毫秒) ≤ 1.5 * cluster-node-timeout + 1000
				
				cluster-node-timeout的默认值为15000ms(15s)，因此故障转移时间会在20s量级。

	- 集群-codis

	- 总结

		1.主从依靠全量复制，部分复制和之后的命令传播  
		
		2.哨兵系统依靠哨兵三个定时任务  
		
		3.集群系统依靠Gossip协议完成节点状态同步

- [数据结构及存储细节](https://yq.aliyun.com/articles/67122)

	- 基本的结构体RedisObject

		所有Redis对象都被封装在RedisObject中
		
		（1）dictEntry：Redis是Key-Value数据库，因此对每个键值对都会有一个dictEntry，里面存储了指向Key和Value的指针；next指向下一个dictEntry，与本Key-Value无关。  
		
		（2）Key：图中右上角可见，Key（”hello”）并不是直接以字符串存储，而是存储在SDS结构中。  
		
		（3）redisObject：Value(“world”)既不是直接以字符串存储，也不是像Key一样直接存储在SDS中，而是存储在redisObject中。实际上，不论Value是5种类型的哪一种，都是通过redisObject来存储的；而redisObject中的type字段指明了Value对象的类型，ptr字段则指向对象所在的地址。不过可以看出，字符串对象虽然经过了redisObject的包装，但仍然需要通过SDS存储

		- 衍生

			- string

				1）Set key value
				2）Get key
				3）Getset key value: 获取值并重写key
				4）incr key
				5）Decr key
				6）Incrby key increment
				7）Decrby key decrement
				8）Append key value-

				- 存储细节

					String在redis内部存储默认就是一个字符串（以sds结构存储），被redisObject所引用，当遇到incr、decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int

					- 当值是Numeric

						不需要ptr指到字符空间，直接用指针的值代表字符串的值，此时存储结构就不是sds

				- 衍生

					- bitMap

						通过一个bit位来表示某个元素对应的值或者状态，其中的index就是对应元素本身。Bitmaps本身不是一种数据结构，实际上它是字符串，但它可以对字符串的位进行操作
						
						1) SETBIT key offset value  
						2) GETBIT key offset  
						3) BITCOUNT key start 获取Bitmaps 指定范围值为 1 的位个数  
						4) BITOP operation destkey key [key ...]  
						说明：对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey。  
						BITOP AND destkey key [key ...] ，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。BITOP OR destkey key [key ...] ，对一个或多个 key 求逻辑或，并将结果保存到 destkey 。  
						BITOP XOR destkey key [key ...] ，对一个或多个 key 求逻辑异或，并将结果保存到 destkey。BITOP NOT destkey key ，对给定 key 求逻辑非，并将结果保存到 destkey 。  
						除了 NOT 操作之外，其他操作都可以接受一个或多个 key 作为输入。  
						5) BITPOS key bit [start][end] 返回字符串里面第一个被设置为 1 或者 0 的bit位。

						- 用户签到

							第一种方法：key是签到日期，下标是用户ID。不适合用户跨度很大，用户ID不是纯数字的场景
							
							第二种方法：key是用户ID，下标是签到日期  
							
							Jedis redis = new Jedis("192.168.31.89",6379,100000);  
							//用户uid  
							String uid = "1";  
							String cacheKey = "sign_"+Integer.valueOf(uid);  
							//记录有uid的key  
							// $cacheKey = sprintf("sign_%d", $uid);  
							
							//开始有签到功能的日期  
							String startDate = "2017-01-01";  
							
							//今天的日期  
							String todayDate = "2017-01-21";  
							
							//计算offset(时间戳)  
							long startTime = dateParase(startDate,"yyyy-MM-dd").getTime();  
							long todayTime = dateParase(todayDate,"yyyy-MM-dd").getTime();  
							long offset = (long) Math.floor((todayTime - startTime) / 86400);  
							
							System.out.println("今天是第"+offset+"天");  
							
							//签到  
							//一年一个用户会占用多少空间呢？大约365/8=45.625个字节，好小，有木有被惊呆？  
							redis.setbit(cacheKey,offset,"1");  
							
							//查询签到情况  
							boolean bitStatus = redis.getbit(cacheKey, offset);  
							//判断是否已经签到  
							//计算总签到次数  
							long qdCount = redis.bitcount(cacheKey);

						- 统计活跃

							时间作为key，用户ID作为下标  
							
							Map<String,List<Integer>>dateActiveuser = new HashMap<>();  
							Jedis redis = new Jedis("192.168.31.89",6379,100000);  
							Integer[] temp01 = {1,2,3,4,5,6,7,8,9,10};  
							List<Integer>temp01List = new ArrayList<>();  
							Collections.addAll(temp01List,temp01);  
							dateActiveuser.put("2017-01-10",temp01List);  
							
							
							Integer[] temp02 = {1,2,3,4,5,6,7,8};  
							List<Integer>temp02List = new ArrayList<>();  
							Collections.addAll(temp02List,temp02);  
							dateActiveuser.put("2017-01-11",temp02List);  
							
							Integer[] temp03 = {1,2,3,4,5,6};  
							List<Integer>temp03List = new ArrayList<>();  
							Collections.addAll(temp03List,temp03);  
							dateActiveuser.put("2017-01-12",temp03List);  
							
							Integer[] temp04 = {1,4,5,6};  
							List<Integer>temp04List = new ArrayList<>();  
							Collections.addAll(temp04List,temp04);  
							dateActiveuser.put("2017-01-13",temp04List);  
							
							Integer[] temp05 = {1,4,5,6};  
							List<Integer>temp05List = new ArrayList<>();  
							Collections.addAll(temp05List,temp05);  
							dateActiveuser.put("2017-01-14",temp05List);  
							
							String date[] = {"2017-01-10","2017-01-11","2017-01-12","2017-01-13","2017-01-14"};  
							
							//测试数据放入redis中  
							for (int i=0;i<date.length;i++){  
							    for (int j=0;j<dateActiveuser.get(date[i]).size();j++){  
							        redis.setbit(date[i], dateActiveuser.get(date[i]).get(j), "1");  
							    }  
							}  
							
							//bitOp  
							redis.bitop(BitOP.AND, "stat", "stat_2017-01-10", "stat_2017-01-11","stat_2017-01-12");  
							
							System.out.println("总活跃用户："+redis.bitcount("stat"));  
							
							redis.bitop(BitOP.AND, "stat1", "stat_2017-01-10", "stat_2017-01-11","stat_2017-01-14");  
							System.out.println("总活跃用户："+redis.bitcount("stat1"));  
							
							redis.bitop(BitOP.AND, "stat2", "stat_2017-01-10", "stat_2017-01-11");  
							System.out.println("总活跃用户："+redis.bitcount("stat2"));

						- 存储结构

							本身是字符串，只存0和1

			- list

				1) lpush key value1 value2 …  
				2) rpush key value1 value2 …  
				3) lpushx key value：key存在的前提下将value插入头部
				4) rpushx key value：👆
				5) lrange key start end：获取value从start开始到end结束
				6) lpop key：弹栈
				7) rpop key：👆
				8) rpoplpush resource destination：把尾部元素弹出并添加到头部
				9) llen key：返回key的value数量  
				10) lset key index value：添加value并设置下标，会替换原先下标位置的元素  
				11) lrem key count value：删除count个值为value的元素，如果count大于0，从头向尾遍历并删除count个值为value的元素，如果count小于0，则从尾向头遍历并删除。如果count等于0，则删除链表中所有等于value的元素

				- 基于压缩列表zipList

					是由一系列特殊编码的连续内存块，但是进行修改或增删操作时，复杂度较高；因此当节点数量较少时，可以使用压缩列表；但是节点数量多时，还是使用双端链表划算

				- 基于双端链表

			- hash

				1）hset key field value
				2）hget key field
				3）hgetall key：获取所有键值对  
				4）hexists key field：判断key中的filed是否存在  
				5）hlen key：获取key所包含的field的数量  
				6）hdel key field [field…]：删除filed，返回删除个数  
				7）hkeys key：获取key的所有field  
				8）hvals key：获取key的所有field的value

				- 基于哈希表

					Redis的Hash实际是内部存储的Value为一个HashMap
					
					注意：当HashMap的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，这时对应的value的redisObject的encoding为zipmap，当成员数量增大时会自动转成真正的HashMap,此时encoding为ht

				- 基于压缩列表zipList

				- 衍生

					- SortedSet

						1）zadd key score member score2 member2
						
						2）zrange key
						
						3）zrem key
						
						4）zcard key
						
						内部使用HashMap和跳表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率

						- 基于压缩列表zipList

						- [基于跳表skipList](https://lotabout.me/2018/skip-list/)

						- 可应用延时队列

							- 延时队列

								- 定时器轮询遍历数据库

								- JDK的DelayQueue

								- JDK的ScheduledExecutorService

								- 时间轮（Netty）

									- 原始时间轮

										如图一个轮子，有8个“槽”，可以代表未来的一个时间。如果以秒为单位，中间的指针每隔一秒钟转动到新的“槽”上面，就好像手表一样。如果当前指针指在1上面，我有一个任务需要4秒以后执行，那么这个执行的线程回调或者消息将会被放在5上。那如果需要在20秒之后执行怎么办，由于这个环形结构槽数只到8，如果要20秒，指针需要多转2圈。位置是在2圈之后的5上面（20 % 8 + 1）。这个圈数需要记录在槽中的数据结构里面。这个数据结构最重要的是两个指针，一个是触发任务的函数指针，另外一个是触发的总第几圈数。时间轮可以用简单的数组或者是环形链表来实现  
										
										插入和移除的复杂度都是O(1)  
										
										基于Netty的HashedWheelTimer可以很好实现

									- 分层时间轮

								- 利用分布式调度框架qiartz等定时任务

								- redis的ZSet

									zrangebyscore key min max withscores limit offset count：返回分数 min 到 max 的成员并按照分数从小到大排序, limit 是从 offset 开始展示几个元素。

								- RabbitMq

									AMQP和RabbitMQ本身没有直接支持延迟队列功能，但是可以通过以下特性模拟出延迟队列的功能。
									但是我们可以通过RabbitMQ的两个特性来曲线实现延迟队列：
									
									Time To Live(TTL)  
									RabbitMQ可以针对Queue和Message设置 x-message-tt，来控制消息的生存时间，如果超时，则消息变为dead letter
									RabbitMQ针对队列中的消息过期时间有两种方法可以设置。
									
									A: 通过队列属性设置，队列中所有消息都有相同的过期时间。
									B: 对消息进行单独设置，每条消息TTL可以不同。
									如果同时使用，则消息的过期时间以两者之间TTL较小的那个数值为准。消息在队列的生存时间一旦超过设置的TTL值，就成为dead letter
									Dead Letter Exchanges（DLX）
									RabbitMQ的Queue可以配置x-dead-letter-exchange 和x-dead-letter-routing-key（可选）两个参数，如果队列内出现了dead letter，则按照这两个参数重新路由。
									
									x-dead-letter-exchange：出现dead letter之后将dead letter重新发送到指定exchange
									
									x-dead-letter-routing-key：指定routing-key发送
									
									队列出现dead letter的情况有：
									
									消息或者队列的TTL过期
									队列达到最大长度  
									消息被消费端拒绝（basic.reject or basic.nack）并且requeue=false
									利用DLX，当消息在一个队列中变成死信后，它能被重新publish到另一个Exchange。这时候消息就可以重新被消费。
									
									
									总结：消息在延时时间后死亡，会被重新捞起发送到另外的交换机

						- 衍生

							- geo

								本质上是借助ZSET，并且使用GeoHash技术进行填充。Redis中将经纬度使用52位的整数进行编码，放进zset中，score就是GeoHash的52位整数值。  
								
								1）向cars:locations中增加车辆编号为1以及车辆编号为2的位置信息
								geoadd cars:locations 120.346111 31.556381 1 120.375821 31.560368 2   
								
								2）获取车辆编号为1的车辆位置信息
								geopos cars:locations 1  
								
								3）获取编号为1的车辆与编号为2的车辆之间的距离
								geodist cars:locations 1 2 km  
								
								4）以经度120.375821纬度31.556381为中心查找5公里范围内的车辆
								GEORADIUS cars:locations 120.375821 31.556381 5 km WITHCOORD WITHDIST WITHHASH  ASC COUNT 100  
									WITHDIST ： 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 距离的单位和用户给定的范围单位保持一致。
								
									WITHCOORD ： 将位置元素的经度和维度也一并返回。
								
									WITHHASH ： 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。
								命令默认返回未排序的位置元素。 通过以下两个参数， 用户可以指定被返回位置元素的排序方式：
								
									ASC ： 根据中心的位置， 按照从近到远的方式返回位置元素。DESC ： 根据中心的位置， 按照从远到近的方式返回位置元素。
								
									在默认情况下， GEORADIUS 命令会返回所有匹配的位置元素。 虽然用户可以使用 COUNT  选项去获取前 N 个匹配元素， 但是因为命令在内部可能会需要对所有被匹配的元素进行处理， 所以在对一个非常大的区域进行搜索时， 即使只使用 COUNT 选项去获取少量元素， 命令的执行速度也可能会非常慢。 但是从另一方面来说， 使用 COUNT 选项去减少需要返回的元素数量， 对于减少带宽来说仍然是非常有用的。
								
								5）以编号为2的车辆为中心查找5公里范围内的车辆 GEORA  
								GEORADIUSBYMEMBER cars:locations 2 5 km WITHCOORD WITHDIST WITHHASH  ASC COUNT 100

								- 附近的人

								- 存储结构

									底层基于zset实现，由于GEO没有删除指令，可以借助zset实现删除操作
									
									1）遍历集合
									ZRANGE cars:locations  0 -1 WITHSCORES  
									
									2）添加元素
									ZADD cars:locations 4054421060663027 1

								- geohash

					- set

						1）sadd key value1、value2…
						2）smembers key：获取所有元素  
						3）scard key：获取元素个数  
						4）sismember key member：判断member是否存在  
						5）srem key member1：删除  
						6）srandmember key：随机返回元素  
						7）sdiff sdiff key1 key2：返回key1与key2中相差的成员，而且与key的顺序有（类似sql的左连接右连接）  
						8）sdiffstore destination key1 key2：将key1、key2相差的成员存储在destination  
						9）sinter key[key1, key2…]：返回交集  
						10）sinterstore destination key1 key2：将交集存储在destination  
						11）sunion key1、key2：返回并集  
						12）sunionstore destination key1 key2：将并集存储在destination

						- 基于整数集合intset

							typedef struct intset{  
							    uint32_t encoding;  
							    uint32_t length;  
							    int8_t contents[];  
							} intset;  
							
							看起来是数组

						- 基于哈希表

							set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因

			- Pipeline

				可以批量执行一组指令，一次性返回全部结果，可以减少频繁的请求应答

			- HyperLogLog

				供不精确的去重计数功能，比较适合用来做大规模数据的去重统计，例如统计 UV

			- 通用操作

				1）keys pattern
				2）del key
				3）exist key
				4）rename key newKey
				5）expire key
				6）ttl key：获取key所剩过期时间，如果没设置，返回-1，如果不存在，返回-2
				7）type key：key类型
				8）dbsize：打印key数量
				9）flushall：删除所有现有的数据库  
				10）flushdb：删除当前db

	- [基于哈希表的字典](https://zhuanlan.zhihu.com/p/70979891)

		Redis 是一个键值对数据库， 数据库中的键值对由字典保存： 每个数据库都有一个对应的字典， 这个字典被称之为键空间（key space）。  
		
		当用户添加一个键值对到数据库时（不论键值对是什么类型）， 程序就将该键值对添加到键空间； 当用户从数据库中删除键值对时， 程序就会将这个键值对从键空间中删除  
		
		Redis采用拉链法解决hash冲突

		- 字典

			typedef struct dict{  
			   //类型特定函数
			   dictType *type;  
			   //私有数据
			   void *privdata;  
			   //哈希表
			   dictht ht[2];  
			   //rehash索引
			   //当rehash不在进行时，值为-1
			   int trehashidex  
			}  
			type属性和privdata属性是针对不同类型的键值对，为创建多态字典而设置的。
			
			type属性是一个指向dictType结构的指针，每个dictType结构保存了一簇用于操作特定类型键值对的函数，Redis会为不同用途的字典设置不同类型的特定函数

			- 渐进式扩容

				1.为ht[1]分配空间
				2.将rehashidx初始化为0 ，代表rehash工作正式开始
				3.每次字典进行删除、查找、更新操作时， 会同时在两个hash表上进行（先查找ht[0], 如果没找到，再去查找ht[1]）。 进行添加操作时，会直接添加到ht[1]
				4.在进行每次增删改查操作时， 会同时把ht[0]在rehashidx索引上的所有键值对都rehash到ht[1]上，完成后rehashidx加1
				5.当ht[0]所有元素都被复制到ht[1]， 设置rehashidx的值为-1
				6.回收ht[0]

		- 哈希表

			typedef struct dictht{  
			    //哈希表数组
			    dictEntry **table;  
			    //哈希表大小
			    unsigned long size;  
			    unsigned long sizemask;  
			    //该hash表已有节点数量
			    unsigned long used;  
			}

		- 哈希表节点

			typedef struct dictEntry{  
			   void *key;  
			   union{  
			      void *val;  
			      uint_64_tu64;  
			      int64_ts64;  
			     } v  
			     struct dictEntry *next;  
			}dictEntry;  
			key属性保存着键值对中的键，而v属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者一个整数。
			next属性是指向下一个哈希表节点，这个指针可以将多个哈希值相同的键值对链接在一起，以此来解决键冲突问题

- 持久化机制

	- RDB

		在指定的时间间隔内将内存中的数据集快照写入磁盘，生成二进制文件，文件名是dump.rdb

		- 触发方式

			- 自动

				 save 900 1     #900秒内如果超过1个key被修改
				 save 300 10    #300秒内容如超过10个key被修改
				 save 60 10000 #60秒内如果至少有 10000个key的值变化

			- 手动

				- 同步save

					阻塞当前Redis服务器，期间不处理其他命令，直到RDB过程完成为止

				- 异步besave

					fork：redis通过创建子进程来进行bgsave操作，该过程会短暂阻塞。基本上Redis内部所有的RDB操作都是采用bgsave命令
					cow：copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来

		- 配置说明

			redis.conf的SNAPSHOTTING下
			①、save：这里是用来配置触发 Redis的 RDB 持久化条件，也就是什么时候将内存中的数据保存到硬盘。比如“save m n”。表示m秒内数据集存在n次修改时  
			
			②、stop-writes-on-bgsave-error ：默认值为yes。当启用了RDB且最后一次后台保存数据失败，Redis是否停止接收数据。这会让用户意识到数据没有正确持久化到磁盘上，否则没有人会注意到灾难发生了。如果Redis重启了，那么又可以重新开始接收数据了  
			
			③、rdbcompression ；默认值是yes。对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话，redis会采用LZF算法进行压缩。如果你不想消耗CPU来进行压缩的话，可以设置为关闭此功能，但是存储在磁盘上的快照会比较大。  
			
			④、rdbchecksum ：默认值是yes。在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能。  
			
			⑤、dbfilename ：设置快照的文件名，默认是 dump.rdb  
			
			⑥、dir：设置快照文件的存放路径，这个配置项一定是个目录，而不能是文件名。默认是和当前配置文件保存在同一目录

		- 优缺点

			优势：  
			1.一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这样非常方便进行备份。可以很容易的将一个一个RDB文件移动到其他的存储介质上
			2.RDB在恢复大数据集时的速度比AOF的恢复速度要快。
			3.RDB可以最大化Redis的性能：父进程在保存RDB文件时唯一要做的就是fork出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘I/O操作
			
			劣势：  
			1.如果你需要尽量避免在服务器故障时丢失数据，那么RDB不适合你。 虽然Redis允许你设置不同的保存点(save point)来控制保存RDB文件的频率， 但是， 因为RDB文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少5分钟才保存一次RDB文件。 在这种情况下， 一旦发生故障停机， 你就可能会丢失好几分钟的数据。  
			2.每次保存RDB的时候，Redis都要fork()出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时，fork()可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且CPU时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然AOF重写也需要进行fork() ，但无论AOF重写的执行间隔有多长，数据的耐久性都不会有任何损失

		- 数据恢复

			将备份文件(dump.rdb)移动到 redis 安装目录并启动服务即可，redis就会自动加载文件数据至内存了。Redis 服务器在载入RDB文件期间，会一直处于阻塞状态，直到载入工作完成为止

	- 比较和选择

		如果可以忍受一小段时间内数据的丢失，毫无疑问使用 RDB 是最好的，定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快，而且使用 RDB 还可以避免 AOF 一些隐藏的 bug；否则就使用 AOF 重写。但是一般情况下建议不要单独使用某一种持久化机制，而是应该两种一起用，在这种情况下,当redis重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整

	- AOF

		redis会将每一个写命令都通过write函数追加到文件中(默认appendonly.aof)

		- 配置说明

			redis.conf的APPEND ONLY MODE
			①、appendonly：默认值为no，也就是说redis 默认使用的是rdb方式持久化，如果想要开启 AOF 持久化方式，需要将 appendonly 修改为 yes。
			②、appendfilename ：aof文件名，默认是"appendonly.aof"
			③、appendfsync：aof持久化策略的配置；
			　no表示不执行fsync，由操作系统保证数据同步到磁盘，速度最快，但不太全
			　always表示每次写入都执行fsync，以保证数据同步到磁盘，效率很低
			　everysec表示每秒执行一次fsync，可能会导致丢失这1s数据
			   通常选择 everysec ，兼顾安全性和效率
			④、no-appendfsync-on-rewrite：在aof重写或者写入rdb文件的时候，会执行大量IO，此时对于everysec和always的aof模式来说，执行fsync会造成阻塞过长时间，no-appendfsync-on-rewrite字段设置为默认设置为no。如果对延迟要求很高的应用，这个字段可以设置为yes，否则还是设置为no，这样对持久化特性来说这是更安全的选择。设置为yes表示rewrite期间对新写操作不fsync,暂时存在内存中,等rewrite完成后再写入，默认为no，建议yes。Linux的默认fsync策略是30秒。可能丢失30秒数据。
			⑤、auto-aof-rewrite-percentage：默认值为100。aof自动重写配置，当目前aof文件大小超过上一次重写的aof文件大小的百分之多少进行重写，即当aof文件增长到一定大小的时候，Redis能够调用bgrewriteaof对日志文件进行重写。当前AOF文件大小是上次日志重写得到AOF文件大小的二倍（设置为100）时，自动启动新的日志重写过程。
			⑥、auto-aof-rewrite-min-size：64mb。设置允许重写的最小aof文件大小，避免了达到约定百分比但尺寸仍然很小的情况还要重写。
			⑦、aof-load-truncated：aof文件可能在尾部是不完整的，当redis启动的时候，aof文件的数据被载入内存。重启可能发生在redis所在的主机操作系统宕机后，尤其在ext4文件系统没有加上data=ordered选项，出现这种现象  redis宕机或者异常终止不会造成尾部不完整现象，可以选择让redis退出，或者导入尽可能多的数据。如果选择的是yes，当截断的aof文件被导入的时候，会自动发布一个log给客户端然后load。如果是no，用户必须手动redis-check-aof修复AOF文件才可以。默认值为 yes

		- 优缺点

			优势：  
			使用AOF持久化会让Redis变得非常耐久（much more durable）：你可以设置不同的fsync策略，比如无fsync，每秒钟一次fsync ，或者每次执行写入命令时fsync。AOF 的默认策略为每秒钟 fsync一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。
			
			AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。
			Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。
			
			AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态
			
			劣势：  
			对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。  
			
			根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）  
			
			AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的

		- rewrite

			通过redis.conf配置文件中的auto-aof-rewrite-percentage：默认值为100，以及auto-aof-rewrite-min-size：64mb 配置，也就是说默认Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发。
			    策略：  
			　　①子进程进行 AOF 重写期间，服务器进程（父进程）可以继续处理其他命令
			　　②子进程带有父进程的数据副本，使用子进程而不是线程，可以在避免使用锁的情况下，保证数据的安全性  
			
			使用子进程解决了上面的问题，但是新问题也产生了：因为子进程在进行 AOF 重写期间，服务器进程依然在处理其它命令，这新的命令有可能也对数据库进行了修改操作，使得当前数据库状态和重写后的 AOF 文件状态不一致。
			
			为了解决这个数据状态不一致的问题，Redis 服务器设置了一个 AOF 重写缓冲区，这个缓冲区是在创建子进程后开始使用，当Redis服务器执行一个写命令之后，就会将这个写命令也发送到 AOF 重写缓冲区。当子进程完成 AOF 重写之后，就会给父进程发送一个信号，父进程接收此信号后，就会调用函数将 AOF 重写缓冲区的内容都写到新的 AOF 文件中

		- 触发方式

			- 自动

				根据 auto-aof-rewrite-min-size 和 auto-aof-rewrite-percentage 配置项，以及 aof_current_size 和 aof_base_size 的状态确定触发时机
				
				auto-aof-rewrite-min-size：执行 AOF 重写时，文件的最小体积，默认值为 64MB
				auto-aof-rewrite-percentage：执行 AOF 重写时，当前 AOF 大小（aof_current_size）和上一次重写时 AOF 大小（aof_base_size）的比值

			- 手动

				直接调用 bgrewriteaof 命令，该命令的执行与 bgsave 有些类似，都是 fork 子进程进行具体的工作，且都只有在 fork 时会阻塞

	- 恢复机制

		1.如果只配置 AOF，重启时加载AOF文件恢复数据
		2.如果同时配置了RDB和AOF，启动优先使用AOF日志来恢复数据
		3.如果只配置RDB，启动是将加载dump文件恢复数据

- 整合spring

	- redis事务

		1）事务执行期间不为其他客户端服务
		2）事务中某条命令出错，其后命令依然会执行

		- MULTI和EXEC两个命令

	- spring-redisTemplate

		- redisTemplate.opsForValue()

		- redisTemplate.opsForHash()

		- redisTemplate.opsForList()

		- redisTemplate.opsForSet()

		- redisTemplate.opsForZSet()

		- 专门字符串类

			- StringRedisTemplate

- 过期策略及内存淘汰

	- 定期删除

		- 

			- 每N毫秒随机检查是否有过期的key

	- 惰性删除

		- 当访问某个key时，会先检查一下key是否过期，如果过期了则会删除

	- 内存淘汰机制

		1）volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰。
		（2）volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
		（3）volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
		（4）allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰
		（5）allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰
		（6）no-enviction：禁止淘汰数据，当内存不足以容纳新写入数据时，新写入操作直接报错

		- 淘汰算法

			第一步：依次遍历所有db  
			第二步：根据淘汰策略挑选一个Key进行淘汰；若策略是lru或ttl，采用近似算法，随机取n个样本（默认为5，可配置），从中选出最佳值  
			第三步：遍历完后，计算当前使用内存量是否超过允许值；若是，则继续步骤1

- Redis与消息服务

	- 观察者模式

		list的操作BLPOP或BRPOP，列表的阻塞式弹出

	- Pub/Sub模式

		在消息A入队list的同时发布（PUBLISH）消息B到频道channel，此时已经订阅channel的worker就接收到了消息B，知道了list中有消息A进入，即可循环lpop或rpop来消费list中的消息服务器定时或周期性执行的事件例如，定期执行RDB持久化

- Redis与Reactor

	Redis服务器用主线程执行I/O多路复用程序文件事件分派器以及事件处理器而且，尽管多个文件事件可能会并发出现，Redis服务器是顺序处理各个文件事件的

	- 文件事件

		Redis客户端通过socket与Redis服务器连接，而文件事件就是服务器对套接字操作的抽象例如，客户端发了一个GET命令请求，对于Redis服务器来说就是一个文件事件

	- 时间事件

		服务器定时或周期性执行的事件例如，定期执行RDB持久化

	- 多路IO复用

		多路 I/O 复用模型是利用select、poll、epoll可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有I/O事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗），且Redis在内存中操作数据的速度非常快（内存内的操作不会成为这里的性能瓶颈），主要以上两点造就了Redis具有很高的吞吐量

- 面试题

	- 为何Redis运行速度快

		1.基于内存
		2.单进程单线程省去线程切换
		3.IO多路复用
		4.数据备份和恢复是多线程的

	- 缓存穿透

		原因：访问不存在的Key
		
		方案：  
		1.布隆过滤器  
		2.缓存空结果，设置较短的过期时间

		- 使用布隆过滤器

			布隆过滤器存储redis中的key，其特点是可能有，没有就没有

	- 缓存击穿

		原因：刚好到期的key被高并发访问
		
		方案：  
		1.使用互斥锁  
		
		2.失效时随机sleep很短时间，再次查询，如果失败再执行更新  
		
		3.限流组件hystrix

	- 缓存雪崩

		原因：缓存同时间大量失效  
		
		方案：过期时间附加随机数，分散过期时间

	- 热点key问题

		1.缓存每命中一次，就给命中的缓存增加一定ttl(过期时间)(根据具体情况来设定, 比如10分钟)。一段时间后, 热数据的ttl都会较大，不会自动失效，而冷数据基本上过了设定的ttl就马上失效了  
		
		2.客户端热点key缓存：将热点key对应value并缓存在客户端本地，并且设置一个失效时间。对于每次读请求，将首先检查key是否存在于本地缓存中，如果存在则直接返回，如果不存在再去访问分布式缓存的机器。（可以用ecache，集合类）  
		
		3.将热点key分散为多个子key，然后存储到缓存集群的不同机器上，这些子key对应的value都和热点key是一样的。当通过热点key去查询数据时，通过某种hash算法随机选择一个子key，然后再去访问缓存机器，将热点分散到了多个子key上。

	- [缓存一致性问题](https://juejin.im/post/5d7c7a14f265da03f47c4f93)

		先更新数据库，再删除缓存  
		
		解读：  
		1.为什么不更新缓存？  
		防止两个并发写操作，写操作1更新缓存慢过写操作2，导致缓存脏数据  
		
		2.会有什么问题？  
		（1）缓存刚好失效  
		（2）请求A查询数据库，得一个旧值  
		（3）请求B将新值写入数据库  
		（4）请求B删除缓存  
		（5）请求A将查到的旧值写入缓存  
		
		3.删除缓存失败怎么办？  
		把删除失败的结果发送到MQ，让MQ不断尝试删除

	- mencache和redis区别

		1.MC处理请求时使用多线程异步IO的方式，可以合理利用CPU多核的优势，性能非常优秀  
		2.MC可以对缓存的数据设置失效期，过期后的数据会被清除  
		3.失效的策略采用延迟失效，就是当再次使用数据时检查是否失效；  
		3.当容量存满时，会对缓存中的数据进行剔除，剔除时除了会对过期key进行清理，还会按LRU策略对数据进行剔除（改过程存在钙化问题）  
		4.key不能超过250个字节，最大失效时间是30天，value不能超过1M字节，只支持K-V结构，不提供持久化和主从同步功能。  
		
		mencache与redis区别  
		1.MC多线程异步处理客户端请求，Reds单线程处理客户端请求  
		2.Redis支持持久化，支持丰富的数据结构  
		3.Redis提供主从同步机制，以及Cluster集群部署能力，能够提供高可用服务

	- 防止脑裂和数据丢失

### MongoDB

### 缓存一致性

- Cache Alide Pattern

	缓存旁路策略：  
	1.失效：程序先从缓存中读取数据，如果没有命中，则从数据库中读取，成功之后将数据放到缓存中  
	2.命中：程序先从缓存中读取数据，如果命中，则直接返回  
	3.更新：程序先更新数据库，在删除缓存

### Ehcache

基于map的java进程框架

## 分布式消息服务

### JMS

Java Message Service，实际上是指JMS API

支持消息类型：  
TextMessage : javax.jms.TextMessage，表示一个文本对象

ObjectMessage : javax.jms.ObjectMessage，表示一个JAVA对象

BytesMessage : javax.jms.BytesMessage，表示字节数据

StreamMessage :javax.jms.StreamMessage，表示java原始值数据流

MapMessage : javax.jms.MapMessage，表示键值对

- 通用规范

	public class JMSDemo {  
	        ConnectionFactory connectionFactory;  
	        Connection connection;  
	        Session session;  
	        Destination destination;  
	        MessageProducer producer;  
	        MessageConsumer consumer;  
	        Message message;  
	        boolean useTransaction = false;  
	        try {  
	                Context ctx = new InitialContext();  
	                connectionFactory = (ConnectionFactory) ctx.lookup("ConnectionFactoryName");  
	                //使用ActiveMQ时：connectionFactory = new ActiveMQConnectionFactory(user, password, getOptimizeBrokerUrl(broker));
	                connection = connectionFactory.createConnection();  
	                connection.start();  
	                session = connection.createSession(useTransaction, Session.AUTO_ACKNOWLEDGE);  
	                destination = session.createQueue("TEST.QUEUE");  
	                //生产者发送消息
	                producer = session.createProducer(destination);  
	                message = session.createTextMessage("this is a test");  
	
	                //消费者同步接收
	                consumer = session.createConsumer(destination);  
	                message = (TextMessage) consumer.receive(1000);  
	                System.out.println("Received message: " + message);  
	                //消费者异步接收
	                consumer.setMessageListener(new MessageListener() {  
	                        @Override  
	                        public void onMessage(Message message) {  
	                                if (message != null) {  
	                                        doMessageEvent(message);  
	                                }  
	                        }  
	                });  
	        } catch (JMSException e) {  
	                ...  
	        } finally {  
	                producer.close();  
	                session.close();  
	                connection.close();  
	        }  
	}

	- 两种消息模型

		- P2P：主动

			1.消息队列（Queue）：每个消息只有一个消费者，且被消费的消息从队列消失
			
			2.发送者（Sender）：送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，它不会影响到消息被发送到队列
			
			3.接收者(Receiver)：接收者在成功接收消息之后需向队列应答成功
			
			总结：每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时

		- Pub/Sub：被动

			1.主题（Topic）：每个消息可以有多个消费者
			
			2.发布者（Publisher）：发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息，而且为了消费消息，订阅者必须保持运行的状态
			
			3订阅者（Subscriber）：为了缓和这样严格的时间相关性，JMS允许订阅者创建一个可持久化的订阅。这样，即使订阅者运行，它也能接收到发布者的消息
			
			总结：客户端以topic为Destination，发布者向topic发送消息，订阅者注册接收来自topic的消息。发送到topic的任何消息都将自动传递给所有订阅者

	- 整合spring

		- Jms每次发送消息都会重新创建连接、会话和生产者，Spring中提供了两种连接池

		- ConnectionFactory(连接工厂)

			- SingleConnectionFactory

			- CachingConnectionFactory

		- JmsTemplate

			发送和接收消息的线程安全模板类

		- MessageListener

			消息监听器

	- 五种消息类型

		TextMessage  主体包含字符串
		
		BytesMessage  主体包含连续字节流
		
		MapMessage  主体包含键值对
		
		StreamMessage主体包含流
		
		ObjectMessage  主体包含对象

- [ActiveMQ](https://www.cnblogs.com/cyfonly/p/6380860.html)

	- 消息存储机制

		1）KahaDB(默认)：ActiveMQ 5.3 版本起的默认存储方式。KahaDB存储是一个基于文件的快速存储消息，设计目标是易于使用且尽可能快。它使用基于文件的消息数据库意味着没有第三方数据库的先决条件
		
		2）AMQ：与 KahaDB 存储一样，AMQ存储使用户能够快速启动和运行，因为它不依赖于第三方数据库。AMQ 消息存储库是可靠持久性和高性能索引的事务日志组合，当消息吞吐量是应用程序的主要需求时，该存储是最佳选择。但因为它为每个索引使用两个分开的文件，并且每个 Destination 都有一个索引，所以当你打算在代理中使用数千个队列的时候，不应该使用它
		
		3）JDBC：选择关系型数据库，通常的原因是企业已经具备了管理关系型数据的专长，但是它在性能上绝对不优于上述消息存储实现。事实是，许多企业使用关系数据库作为存储，是因为他们更愿意充分利用这些数据库资源
		
		4）RAM：内存消息存储器将所有持久消息保存在内存中。在仅存储有限数量 Message 的情况下，内存消息存储会很有用，因为 Message 通常会被快速消耗。在 activema.xml 中将 broker 元素上的 persistent 属性设置为 false 即可

	- 部署模式

		- 单例模式

		- Master-Slave

			- shared filesystem Master-Slave

				通过共享存储目录来实现master和slave的热备，所有的ActiveMQ应用都在不断地获取共享目录的控制权，哪个应用抢到了控制权，它就成为master
				
				多个共享存储目录的应用，谁先启动，谁就可以最早取得共享目录的控制权成为master，其他的应用就只能作为slave

			- shared database Master-Slave

				与shared filesystem方式类似，只是共享的存储介质由文件系统改成了数据库而已

		- Broker-Cluster

			各个broker通过网络互相连接，并共享queue。当broker-A上面指定的queue-A中接收到一个message处于pending状态，而此时没有consumer连接broker-A时。如果cluster中的broker-B上面由一个consumer在消费queue-A的消息，那么broker-B会先通过内部网络获取到broker-A上面的message，并通知自己的consumer来消费

		- [Master-Slave与Broker-Cluster相结合](https://blog.csdn.net/tjcyjd/article/details/78298099)

- [RocketMQ](https://www.cnblogs.com/qdhxhz/p/11094624.html)

	1.天然支持集群，其核心四组件（Name Server、Broker、Producer、Consumer）每一个都可以在没有单点故障的情况下进行水平扩展
	
	2.采用零拷贝原理实现超大的消息的堆积能力
	
	3.顺序消息分为全局有序和局部有序，一般推荐使用局部有序，即生产者通过将某一类消息按顺序发送至同一个队列来实现
	
	4.消息过滤分为在服务器端过滤和在消费端过滤  
	
	5.除了支持普通消息，顺序消息之外还支持事务消息，这个特性对于分布式事务来说提供了又一种解决思路

	- 集群架构

		- NameServer

			1.管理两部分数据：集群的Topic-Queue的路由配置；Broker的实时配置信息  
			2.没有主从之分，nameServer结点之间也不同步数据，broker向所有nameServer上报路由信息  
			3.broker挂掉，nameServer也需要30S才能知道，生产者和消费者无法得知，只能自己定期拉取nameServer信息  
			
			与broker的关联  
			1.单个Broker和所有Name Server保持长连接，定时更新Topic信息到NameServer  
			2.心跳：  
				A）心跳间隔：每隔30秒向所有NameServer发送心跳，心跳包含了自身的Topic配置信息  
				B）心跳超时：NameServer每隔10秒，扫描所有还存活的Broker连接，若某个连接2分钟内没有发送心跳数据，则断开连接  
				C）断开：当Broker挂掉，NameServer会根据心跳超时主动关闭连接，一旦连接断开，会更新Topic与队列的对应关系，但不会通知生产者和消费者  
			
			与producer/consumer的关系  
			1.Producer/Consumer随机与某个节点建立长连接，通过查询接口每30秒获取Topic对应的Broker的地址信息  
			2.Producer在发送消息前会根据Topic到NameServer获取到Broker的路由信息，Consumer也会定时获取Topic的路由信息  
			3.Producer/Consumer与关联的所有broker保存长连接

		- Broker

			1.RocketMQ实例，启动时到NameServer注册，并与所有的NameServer节点保持长连接及心跳（发送心跳时会带上当前自己所负责的所有Topic信息），并会定时将Topic信息注册到NameServer，底层的通信和连接都是基于Netty实现的
			
			2.负责消息存储，存储与消息相关的元数据，包括用户组、消费进度偏移量、队列信息等
			
			3.Broker在RocketMQ中是进行处理Producer发送消息请求，Consumer消费消息的请求，并且进行消息的持久化，以及HA策略和服务端过滤，就是集群中很重的工作都是交给了Broker进行处理

			- 部署

				在主从模式下，master负责写读，slave负责读
				
				1、消费者的系统在获取消息的时候会先发送请求到Master Broker上去，请求获取一批消息，此时Master Broker是会返回一批消息给消费者系统的  
				2、Master Broker在返回消息给消费者系统的时候，会根据当时Master Broker的负载情况和Slave Broker的同步情况，向消费者系统建议下一次拉取消息的时候是从Master Broker拉取还是从Slave Broker拉取  
				
				3.如果Master Broker挂了，可以是用Dledger机制（基于Raft协议实现的一个机制）实现自动故障切换，只要10秒或者几十秒的时间就可以完成

				- 单Master

					这种方式一旦 Broker 重启或宕机会导致整个服务不可用，这种方式风险较大，所以显然不建议线上环境使用

				- 多Master

					所有消息服务器都是 Master ，没有 Slave 。这种方式优点是配置简单，单个 Master 宕机或重启维护对应用无影响。缺点是单台机器宕机期间，该机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受影响

				- 多Master多Slave

					- 异步复制

						每个 Master 配置一个 Slave，所以有多对 Master-Slave，消息采用异步复制方式，主备之间有毫秒级消息延迟。这种方式优点是消息丢失的非常少，且消息实时性不会受影响，Master 宕机后消费者可以继续从 Slave 消费，中间的过程对用户应用程序透明，不需要人工干预，性能同多 Master 方式几乎一样。缺点是 Master 宕机时在磁盘损坏情况下会丢失极少量消息

					- 同步双写

						每个 Master 配置一个 Slave，所以有多对 Master-Slave ，消息采用同步双写方式，主备都写成功才返回成功。这种方式优点是数据与服务都没有单点问题，Master 宕机时消息无延迟，服务与数据的可用性非常高。缺点是性能相对异步复制方式略低，发送消息的延迟会略高

			- 核心配置

				# nameServer地址，如果nameserver是多台集群的话，就用分号分割
				namesrvAddr=172.1.21.29:9876;143.13.262.43:9876  
				# 所属集群名字(同一主从下:Master和slave名称要一致)
				brokerClusterName=rocketmq-cluster  
				# broker名字，注意此处不同的配置文件填写的不一样  例如：在a.properties 文件中写 broker-a  在b.properties 文件中写 broker-b
				brokerName=broker-a  
				# 0 表示 Master，>0 表示 Slave
				brokerId=0  
				# Broker 对外服务的监听端口
				listenPort=10911  
				# 在发送消息时，自动创建服务器不存在的topic，默认创建的队列数。由于是4个broker节点，所以设置为4
				# defaultTopicQueueNums=4  
				# 是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
				autoCreateTopicEnable=true  
				# 是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
				autoCreateSubscriptionGroup=true  
				# commitLog每个文件的大小默认1G
				mapedFileSizeCommitLog=1073741824  
				# ConsumeQueue每个文件默认存30W条，根据业务情况调整
				mapedFileSizeConsumeQueue=300000  
				# 检测可用的磁盘空间大小,当磁盘被占用超过90%，消息写入会直接报错                    
				diskMaxUsedSpaceRatio=90  
				# Broker 的角色: ASYNC_MASTER 异步复制Master ; SYNC_MASTER 同步双写Master; SLAVE
				brokerRole=SYNC_MASTER  
				# 刷盘方式 ASYNC_FLUSH 异步刷盘; SYNC_FLUSH 同步刷盘
				flushDiskType=ASYNC_FLUSH

			- 两种持久化机制

				- 同步刷盘

					当数据成功写到内存中之后立刻刷盘，在保证消息写到磁盘也成功的前提下返回写成功状态  
					
					具体流程是，消息写入内存的PAGECACHE后，立刻通知刷盘线程刷盘，然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，返回消息写成功的状态。  
					一般只用于金融场景。

				- 异步刷盘

					在返回写成功状态时，消息可能只是被写入了内存的PAGECACHE，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘操作，快速写入。

			- 思考

				思考1一旦某个broker master宕机，生产者和消费者多久才能发现？
				
				受限于Rocketmq的网络连接机制，默认情况下最多需要30秒，因为消费者每隔30秒从nameserver获取所有topic的最新队列情况，这意味着某个broker如果宕机，客户端最多要30秒才能感知。
				
				思考2 master恢复恢复后，消息能否恢复。
				消费者得到Master宕机通知后，转向Slave消费，但是Slave不能保证Master的消息100%都同步过来了，因此会有少量的消息丢失。但是消息最终不会丢的，一旦Master恢复，未同步过去的消息会被消费掉。

			- Message

				一条消息必须有一个主题（Topic），主题可以看做是你的信件要邮寄的地址。
				
				一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 Key 并在 Broker 上查找此消息以便在开发期间查找问题

				- Topic

					可以看做消息的规类，它是消息的第一级类型。比如一个电商系统可以分为：交易消息、物流消息等，一条消息必须有一个 Topic
					
					与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。
					
					一个 Topic 也可以被 0个、1个、多个消费者订阅

					- Queue

						主题被划分为一个或多个子主题，即消息队列。  
						一个 Topic 下可以设置多个消息队列，发送消息时执行该消息的 Topic 
						寻找消息的时候，根据给的top和index快速定位消息

						- Offset

							RocketMQ在存储消息时会为每个Topic下的每个Queue生成一个消息的索引文件，每个Queue都对应一个Offset记录当前Queue中消息条数

						- commit log

							真正存储消息的地方。RocketMQ所有生产者的topic都是往这一个地方存的。超过1个G会重建一个文件

						- ConsumeQueue

							是一个逻辑队列，消息到达 CommitLog 文件后，将异步转发到ConsumeQueue。和Topic下的Queue是一一对应的。消费者是直接和ConsumeQueue打交道。ConsumeQueue记录了消费位点，这个消费位点关联了commitlog的位置。所以即使ConsumeQueue出问题，只要commitlog还在，消息就没丢，可以恢复出来。还可以通过修改消费位点来重放或跳过一些消息 

						- IndexFile

							消息索引文件，主要存储消息 Key 与 Offset 的对应关系。
							消息消费队列是RocketMQ专门为消息订阅构建的索引文件，提高根据主题与消息队 列检索消息的速度 ，另外 RocketMQ 引入了 Hash 索引机制为消息建立索引， HashMap 的设 计包含两个基本点 ： Hash 槽与 Hash 冲突的链表结构

					- Tag

						可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag

				- 消息积压

				- 回溯消息

					回溯消费是指Consumer已经消费成功的消息，由于业务上的需求需要重新消费，要支持此功能，Broker在向Consumer投递成功消息后，消息仍然需要保留。并且重新消费一般是按照时间维度。
					例如由于Consumer系统故障，恢复后需要重新消费1小时前的数据，那么Broker要提供一种机制，可以按照时间维度来回退消费进度。
					RocketMQ支持按照时间回溯消费，时间维度精确到毫秒，可以向前回溯，也可以向后回溯

				- 定时消息

					定时消息是指消息发到Broker后，不能立刻被Consumer消费，要到特定的时间点或者等待特定的时间后才能被消费。
					如果要支持任意的时间精度，在Broker层面，必须要做消息排序，如果再涉及到持久化，那么消息排序要不可避免的产生巨大性能开销。
					RocketMQ支持定时消息，但是不支持任意时间精度，支持特定的level，例如定时5s，10s，1m等

				- 延迟消息

					所有的延迟消息由producer发出之后，都会存放到同一个topic（SCHEDULE_TOPIC_XXXX）下，不同的延迟级别会对应不同的队列序号，当延迟时间到之后，由定时线程读取转换为普通的消息存的真实指定的topic下，此时对于consumer端此消息才可见，从而被consumer消费。
					
					RocketMQ的延迟消息本身有一个很大的缺点，熟悉java自带的Timer类的小伙伴应该知道一个timer对应只有一个线程，然后来处理不同的timeTask，而RockerMQ本身也确实只new了一个Timer，也就是说当同时发送的延迟消息过多的时候一个线程处理速度一定是有瓶颈的，因此在实际项目中使用延迟消息一定不要过多依赖，只能作为一个辅助手段

				- 死信队列

					重试队列在重试16次（默认次数）将消息放入死信队列
					
					死信队列中的数据需要通过新订阅该topic进行消费

				- 重试队列

					消费端，一直不回传消费的结果。rocketmq认为消息没收到，consumer下一次拉取，broker依然会发送该消息。
					
					所以，任何异常都要捕获返回ConsumeConcurrentlyStatus.RECONSUME_LATER，rocketmq会放到重试队列。
					
					重试的消息在延迟的某个时间点（默认是10秒，业务可设置）后，再次投递到这个ConsumerGroup

					- 重试机制

						1.在producer：
						如果是异步发送 那么重试次数只有1次
						对于同步而言，超时异常也是不会再去重试  
						
						2.在consumer：
						1.默认重试次数：Product默认是2次，而Consumer默认是16次。
						2.重试时间间隔：Product是立刻重试，而Consumer是有一定时间间隔的。它照1S,5S,10S,30S,1M,2M····2H进行重试。
						3.Product在异步情况重试失效，而对于Consumer在广播情况下重试失效。

				- 分布式事务

					1.A服务先发送个Half Message给Brock端，消息中携带 B服务 即将要+100元的信息
					
					2.当A服务知道Half Message发送成功后，那么开始第3步执行本地事务
					   
					3.执行本地事务(会有三种情况1、执行成功。2、执行失败。3、网络等原因导致没有响应) 
					
					4.如果本地事务成功，那么Product向Brock服务器发送Commit,这样B服务就可以消费该message
					
					5.如果本地事务失败，那么Product向Brock服务器发送Rollback,那么就会直接删除上面这条半消息
					
					6.如果因为网络等原因迟迟没有返回失败还是成功，那么会执行RocketMQ的回调接口,来进行事务的回查

					- Half Message

						是指暂不能被Consumer消费的消息。Producer 已经把消息成功发送到了 Broker 端，但此消息被标记为暂不能投递状态，处于该种状态下的消息称为半消息。需要 Producer
						
						对消息的二次确认后，Consumer才能去消费它

					- 消息回查

						由于网络闪段，生产者应用重启等原因。导致 Producer 端一直没有对 Half Message(半消息) 进行 二次确认。这是Brock服务器会定时扫描长期处于半消息的消息，会主动询问 Producer端 该消息的最终状态(Commit或者Rollback),该消息即为 消息回查

		- Producer

			在发送消息前Producer通过NameServer获取所有Broker的路由信息，根据负载均衡策略选择将消息发到哪个Broker，然后调用Broker接口提交消息

			- 消息三种发送方式

				- 同步发送

					同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信

				- 异步发送

					异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务

				- 单向发送

					单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集

				- 返回的SendStatus

					SEND_OK  
					
					代表发送成功！但并不保证它是可靠的。要确保不会丢失任何消息，还应启用SYNC_MASTER或SYNC_FLUSH。
					
					SLAVE_NOT_AVAILABLE  
					
					如果Broker的角色是SYNC_MASTER（同步复制）(默认为异步），但没有配置Slave Broker，将获得此状态。
					
					FLUSH_DISK_TIMEOUT  
					
					如果Broker设置为 SYNC_FLUSH(同步刷盘）（默认为ASYNC_FLUSH），并且Broker的syncFlushTimeout（默认为5秒）内完成刷新磁盘，将获得此状态。
					
					FLUSH_SLAVE_TIMEOUT  
					
					如果Broker的角色是SYNC_MASTER(同步复制）（默认为ASYNC_MASTER），并且从属Broker的syncFlushTimeout（默认为5秒）内完成与主服务器的同步，将获得此状态。

			- 生产者组

				生产者组（Producer Group）是一类 Producer 的集合，这类 Producer 通常发送一类消息并且发送逻辑一致，所以将这些 Producer 分组在一起。从部署结构上看生产者通过 Producer Group 的名字来标记自己是一个集群

			- demo

				public class Producer {  
				
				    public static void main(String[] args) throws Exception {  
				        //创建一个消息生产者，并设置一个消息生产者组
				        DefaultMQProducer producer = new DefaultMQProducer("niwei_producer_group");  
				
				        //指定 NameServer 地址
				        producer.setNamesrvAddr("localhost:9876");  
				
				        //初始化 Producer，整个应用生命周期内只需要初始化一次
				        producer.start();  
				
				        for (int i = 0; i < 100; i++) {  
				            //创建一条消息对象，指定其主题、标签和消息内容
				            Message msg = new Message(  
				                    "topic_example_java" /* 消息主题名 */,
				                    "TagA" /* 消息标签 */,
				                    ("Hello Java demo RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* 消息内容 */
				            );  
				
				            //发送消息并返回结果
				            SendResult sendResult = producer.send(msg);  
				
				            System.out.printf("%s%n", sendResult);  
				        }  
				
				        // 一旦生产者实例不再被使用则将其关闭，包括清理资源，关闭网络连接等
				        producer.shutdown();  
				    }  
				}  
				
				示例中用 DefaultMQProducer 类来创建一个消息生产者，通常一个应用创建一个 DefaultMQProducer 对象，所以一般由应用来维护生产者对象，可以其设置为全局对象或者单例。该类构造函数入参 producerGroup 是消息生产者组的名字。
				接下来指定 NameServer 地址和调用 start 方法初始化，在整个应用生命周期内只需要调用一次 start 方法。
				初始化完成后，调用 send 方法发送消息，示例中只是简单的构造了100条同样的消息发送，其实一个 Producer 对象可以发送多个主题多个标签的消息，消息对象的标签可以为空。send 方法是同步调用，只要不抛异常就标识成功。
				最后应用退出时调用 shutdown 方法清理资源、关闭网络连接，从服务器上注销自己，通常建议应用在 JBOSS、Tomcat 等容器的退出钩子里调用 shutdown 方法

		- Consumer

			Consumer通过NameServer获取所有broker的路由信息后，向Broker发送Pull请求来获取消息数据

			- 两种消费者

				public class Consumer {  
				
				    public static void main(String[] args) throws Exception {  
				        //创建一个消息消费者，并设置一个消息消费者组
				        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("niwei_consumer_group");  
				        //指定 NameServer 地址
				        consumer.setNamesrvAddr("localhost:9876");  
				        //设置 Consumer 第一次启动时从队列头部开始消费还是队列尾部开始消费
				        consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);  
				        //订阅指定 Topic 下的所有消息
				        consumer.subscribe("topic_example_java", "*");  
				
				        //注册消息监听器
				        consumer.registerMessageListener(new MessageListenerConcurrently() {  
				            public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> list, ConsumeConcurrentlyContext context) {  
				                //默认 list 里只有一条消息，可以通过设置参数来批量接收消息
				                if (list != null) {  
				                    for (MessageExt ext : list) {  
				                        try {  
				                            System.out.println(new Date() + new String(ext.getBody(), "UTF-8"));  
				                        } catch (UnsupportedEncodingException e) {  
				                            e.printStackTrace();  
				                        }  
				                    }  
				                }  
				                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;  
				            }  
				        });  
				
				        // 消费者对象在使用之前必须要调用 start 初始化
				        consumer.start();  
				        System.out.println("消息消费者已启动");
				    }  
				}  
				
				示例中用 DefaultMQPushConsumer 类来创建一个消息消费者，同生产者一样一个应用一般创建一个 DefaultMQPushConsumer 对象，该对象一般由应用来维护，可以其设置为全局对象或者单例。
				接下来指定 NameServer 地址和设置消费者应用程序第一次启动时从队列头部开始消费还是队列尾部开始消费。
				接着调用 subscribe 方法给消费者对象订阅指定主题下的消息，该方法第一个参数是主题名，第二个擦书是标签名，示例表示订阅了主题名 topic_example_java 下所有标签的消息。
				最主要的是注册消息监听器才能消费消息，示例中用的是 Consumer Push 的方式，即设置监听器回调的方式消费消息，默认监听回调方法中 List 里只有一条消息，可以通过设置参数来批量接收消息。
				最后调用 start 方法初始化，在整个应用生命周期内只需要调用一次 start 方法

				- Pull Consumer

					拉取型消费者，主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型

				- Push Consumer

					推送型消费者，封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息

			- 重平衡

			- 两种消费模式

				- 集群消费（Clustering）

					一个消费者集群共同消费一个主题的多个队列，一个队列只会被一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费

				- 广播消费（Broadcasting）

					发给消费者组中的每一个消费者进行消费

			- 消费者组

				消费者组（Consumer Group）一类 Consumer 的集合名称，这类 Consumer 通常消费同一类消息并且消费逻辑一致，所以将这些 Consumer 分组在一起。消费者组与生产者组类似，都是将相同角色的分组在一起并命名，分组是个很精妙的概念设计，RocketMQ 正是通过这种分组机制，实现了天然的消息负载均衡。消费消息时通过 Consumer Group 实现了将消息分发到多个消费者服务器实例，比如某个 Topic 有9条消息，其中一个 Consumer Group 有3个实例（3个进程或3台机器），那么每个实例将均摊3条消息，这也意味着我们可以很方便的通过加机器来实现水平扩展
				
				同一个消费者组内消息一模一样，所以通过消费者组内的消费者只能消费同一个topic

	- 总结

		1.消费者发送的Message会在Broker中的Queue队列中记录
		2.一个Topic的数据可能会存在多个Broker中
		3.一个Broker存在多个Queue
		4.单个的Queue也可能存储多个Topic的消息
		5.每个Topic在Broker上会划分成几个逻辑队列，每个逻辑队列保存一部分消息数据，但是保存的消息数据实际上不是真正的消息数据，而是指向commit log的消息索引

### AMQP

- RabbitMQ

	跨平台，跨语言，支持byte[]消息类型。
	提供了五种消息模型：  
	(1)direct exchange  
	(2)fanout exchange  
	(3)topic change  
	(4)headers exchange  
	(5)system exchange  
	本质来讲，后四种和JMS的pub/sub模型没有太大差别，仅是在路由机制上做了更详细的划分；

	- 五种消息模型

		- 架构

			- producer

				public class Send {  
				    private final static String QUEUE_NAME = "test_work_queue";  
				
				    public static void main(String[] argv) throws Exception {  
				        // 获取到连接
				        Connection connection = ConnectionUtil.getConnection();  
				        // 获取通道
				        Channel channel = connection.createChannel();  
				        // 声明队列
				        channel.queueDeclare(QUEUE_NAME, false, false, false, null);  
				        // 循环发布任务
				        for (int i = 0; i < 50; i++) {  
				            // 消息内容
				            String message = "task .. " + i;  
				            channel.basicPublish("", QUEUE_NAME, null, message.getBytes());  
				            System.out.println(" [x] Sent '" + message + "'");  
				
				            Thread.sleep(i * 2);  
				        }  
				        // 关闭通道和连接
				        channel.close();  
				        connection.close();  
				    }  
				}

			- Channel

				信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接

			- consumer

				消费过程：  
				1）主动：用RPC从MQ Server获取消息  
				
				2）被动：从MQ Server订阅消息，当queue有消息时，会自动把消息通过该socket(长连接)通道发送出去
				
				注意：每个Consumer都有一个BlockQueue，用于缓存从socket中获取的消息。设置prefetch_count可以控制消费者最多持有多少条unack的消息

			- Binding

				消息队列和交换机形成映射关系

			- Exchange

				不存储消息，接收生产者发送的消息，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型

				- queue

					mq内部维护一个大的消息缓冲区。生产者可以发送消息到一个队列，消费者可以尝试从一个队列接收数据

		- Work消息模型

			一个生产者，多个消费者，但是一个消息只能被一个消费者获取

			- 面试题：避免消息堆积？

				1)采用workqueue，多个消费者监听同一队列
				
				2)接收到消息以后，而是通过线程池，异步消费

		- 基于Exchange的发布/订阅模型

			1、1个生产者，多个消费者
			2、每一个消费者都有自己的一个队列
			3、生产者没有将消息直接发送到队列，而是发送到了交换机
			4、每个队列都要绑定到交换机
			5、生产者发送的消息，经过交换机到达队列，实现一个消息被多个消费者获取的目的

			- Fanout：广播，将消息交给所有绑定到交换机的队列

			- Direct：定向，把消息交给符合指定routing key的队列

			- Topic：通配符，把消息交给符合routing pattern(路由模式)的队列

		- 持久化

			1）消息持久化：  
			
			2）队列持久化：  
			
			3）交换机持久化：

	- 消息发送/确认机制

		- 消息确认机制(ACK)

			- 自动ACK

				消息一旦被接收，消费者自动发送ACK

			- 手动ACK

				消息接收后，不会发送ACK，需要手动调用

		- 消息发送机制

			- 事务机制

				- txSelect()

					将当前channel设置成transaction模式

				- txCommit()

					提交事物

				- txRollback()

					回滚事务

			- Confirm模式

				在生产者设置开启了confirm模式之后，每次写的消息都会分配一个唯一的id，然后如何写入了rabbitmq之中，rabbitmq会给你回传一个ack消息，告诉你这个消息发送OK了；如果rabbitmq没能处理这个消息，会回调nack接口。  
				
				//开启confirm  
				channel.confirm();  
				//发送成功回调  
				public void ack(String messageId){  
					//do something  
				}  
				   
				// 发送失败回调  
				public void nack(String messageId){  
				       //do something  
				}

				- 普通confirm模式

					每发送一条消息后，调用waitForConfirms()方法，等待服务器端confirm

				- 批量confirm模式

					每发送一批消息后，调用waitForConfirms()方法，等待服务器端confirm

				- 异步confirm模式

					提供一个回调方法，服务端confirm了一条或者多条消息后Client端会回调这个方法

		- 确保消息被消费

			- 显式Ack模式，该模式下消费者在ACK之前断线，MQ将重新投放消息

- Spring AMQP

	Spring-amqp是对AMQP协议的抽象实现，而spring-rabbit，是对协议的具体实现，也是目前的唯一实现。底层使用的就是RabbitMQ

	- 注册监听者

		@Component  
		public class Listener {  
		
		    @RabbitListener(bindings = @QueueBinding(  
		            value = @Queue(value = "spring.test.queue", durable = "true"),  
		            exchange = @Exchange(  
		                    value = "spring.test.exchange",  
		                    ignoreDeclarationExceptions = "true",  
		                    type = ExchangeTypes.TOPIC  
		            ),  
		            key = {"#.#"}))  
		    public void listen(String msg){  
		        System.out.println("接收到消息：" + msg);
		    }  
		}  
		@Componet：类上的注解，注册到Spring容器
		@RabbitListener：方法上的注解，声明这个方法是一个消费者方法，需要指定下面的属性：
			•	bindings：指定绑定关系，可以有多个。值是@QueueBinding的数组。@QueueBinding包含下面属性：
			◦	value：这个消费者关联的队列。值是@Queue，代表一个队列
			◦	exchange：队列所绑定的交换机，值是@Exchange类型
			◦	key：队列和交换机绑定的RoutingKey

	- 消息处理模板-AmqpTemplate

- Kafka

	吞吐量大的原因  
	
	1.持久化消息的时候，基于页缓存 + 顺序写入  
	
	2.采用零拷贝技术

	- broker

		配置：  
		broker.id：broker唯一标识
		port：9092
		zookeeper.connect：用于存储元数据
		log.dirs：Kafka把所有的消息都保存到磁盘上，存放这些日志片段的目录是通过 log.dirs指定（相当于partition的目录，名字是topic+partition_id）
		auto.create.topics.enable：是否自动创建主题

		- 消息

			1.FIFO原则，append到磁盘
			2.相同消费者组内每个partition只能由一个consumer消费，同一consumer可以消费不同partition  
			3.每条消息记录都包含一个key, value以及时间戳

			- topic

				- partition

					分区本质是物理机上一个个目录，命令规则是topic-partition_number

					- Segment

						partition物理上由多个segment组成，每个segment由.index(索引文件)和.log(数据文件)。寻key过程：二分法遍历.index，找到.log文件，读取.log文件
						
						过来一个消息就append到segment，当segment达到阈值，关闭该segment，开启新的segment

					- 分区分配算法

						1.将所有 Broker（假设共 n 个 Broker）和待分配的 Partition 排序
						2.将第 i 个 Partition 分配到第（i mod n）个 Broker 上
						3.将第 i 个 Partition 的第 j 个 Replica 分配到第（(i + j) mode n）个 Broker 上

					- partition集群

						Leader负责数据读写，Follower向Leader顺序Fetch数据

						- 心跳机制/存活判断

							判断存活条件：  
							1.基于ZK与broker之间的心跳机制，由ZK维护(基于临时节点)
							2.Follower必须及时复制Leader的消息，不能“落后太多”
							  1）leader维护与其保持同步的replica列表(in-sync replica)
							  2）follower复制的条数落后太多，默认4000条
							  3）follower超过一定时间未向leader发送fetch，默认10000

						- leader故障

							为了保证数据一致性，当Leader挂了之后，kafka的controller默认会从ISR中选择一个replica作为Leader继续工作，选择的条件是：新Leader必须有挂掉Leader的所有数据。
							
							​如果为了系统的可用性，而容忍降低数据的一致性的话，可以将"unclean.leader.election.enable = true" ，开启kafka的"脏Leader选举"。当ISR中没有replica，则会从OSR中选择一个replica作为Leader继续响应请求，如此操作提高了Kafka的分区容忍度，但是数据一致性降低了。

						- broker故障转移

							1.Controller在ZK注册Watch，一旦有Broker宕机，其ZK对应的znode会自动被删除，ZK告知Controller注册的watch，Controller读取最新的幸存的Broker
							2.Controller从ISR选择一个replica为leader（partition leader较少的broker将会更有可能成为新的leader）
							3.RPC告知相关broker，你这个是leader

						- 副本

							1.leader维护一个与其基本保持同步的ISR列表(in-sync Replica)
							2.leader对外提供读写功能，所有请求都必须发送到leader所在broker
							3.follower不对外提供服务，采用拉取的方式从leader拉取消息，写入自身日志系统，完成与leader同步

							- 同步复制/异步复制

								同步：  
								producer通知ZK识别leader
								producer向leader写入消息
								leader收到消息后会把消息写入到本地 log
								follower会从leader那里拉取消息  
								follower向本地写入 log
								follower向leader发送写入成功的消息  
								leader收到ISR中所有的followers发送的消息  
								leader向producer返回写入成功的消息
								
								异步：  
								leader在写入本地log之后，直接向client发送返回commit，不需要等待followers复制完成  
								
								同步异步由生产者acks参数控制

							- ISR

								1.leader维护一个与其基本保持同步的动态ISR列表
								
								2.rerplica.lag.time.max.ms=10000  
								rerplica.lag.max.messages=4000  
								
								3.如果follower达到上面两个极限，会被踢出ISR，若后面追上可以重新加入ISR（过程触发reblance）  
								
								4.只有ISR所有followers都保持了消息，这条消息才算commit。acks参数只是对生产者回应起作用

								- LF水印备份机制

									1.leader会保存两个类型的LEO值，一个是自己的 LEO，另一个是follower的LEO值，意味着follower副本的LEO值会保存两份，一份保存到leader中，另一份在自己这儿
									2.

			- 配置

				num.partitions：新创建的主题需要包含多少个分区
				default.replication.factor：保存消息的副本数  
				log.retention.ms：数据可以保留多久，默认一周  
				log.retention.bytes：一个分区数据保存容量上限  
				log.segment.bytes：单个segment容量上限，默认1g  
				log.segment.ms：单个segment保存时长  
				message.max.bytes：限制单个消息大小，默认1 000 000，超过该大小返回失败  
				retention.ms：主题保存的时长，可以覆盖broker端的全局配置  
				retention.bytes：主题保持上限

			- [三种语义](https://russxia.com/2019/05/24/Kafka%E7%9A%84%E4%B8%89%E7%A7%8D%E6%B6%88%E6%81%AF%E6%8A%95%E9%80%92%E8%AF%AD%E4%B9%89/)

				At most once—最多一次，消息可能会丢失，但不会重复
				At least once—最少一次，消息不会丢失，可能会重复
				producer-broker: 当producer向broker发送消息时，一旦这条消息commit了，由于broker有replication的存在，这条消息就不会丢失，这样就可以在producer-broker端保证at least once消息语义。当然，也可以通过设置Producer异步发送来实现at most once语义。  
				
				broker-consumer: consumer在消费消息时，每个consumer都会在保存offset来记录自己消费的位置(从__consumer_offsets这个topic中取，老版本存储在zk中)。当consumer挂了的时候，就会发生负载均衡，需要consumer group中另外的consumer来接管并继续消费。consumer在处理消息和修改offset时也有两种处理方式:  
				
				consumer读取消息后，先修改offset，然后处理消息。  
				consumer读取消息后，先处理消息，然后修改offset。  
				如果consumer在处理消息的过程中挂掉了，我们无法确认consumer是否已经处理完了消息。所以处理方式一，虽然消息会丢，但消息不会被重复消费，确保了at most once语义。处理方式二，因为consumer还没有修改offset就挂了，所以consumer group中接管的consumer会从上次消费的offset处接手继续消费，虽然消息会重复，但消息肯定不会丢，保证了at least once语义。  
				
				
				Exactly once—只且一次，消息不丢失不重复，只且消费一次
				1.幂等：如果broker配置了enable.idempotence = true,每个producer在初始化的时候都会被分配一个唯一的Producer ID，producer向指定topic的partition发送消息时，携带一个自己维护的自增的Sequence Number。broker会维护一个<pid,topic,partition>对应的seqNum。 每次broker接收到producer发来的消息，会和之前的seqNum做比对，如果刚好大一，则接受;如果相等，说明消息重复;如果相差大于一，则说明中间存在丢消息，拒绝接受。  
				
				这个设计解决了两个问题:  
				
				broker保存消息后，发送ACK前宕机，producer认为没有发送成功并重试，造成消息重复  
				前一条消息发送失败，后一条成功，前一条消息重试后成功，造成消息乱序  
				
				2.事务性保证：上述的幂等操作，只能保证单个producer对于同一个<topic,partition>的exactly once,并不能保证向多个topic partitions写操作时的原子性。更不能保证多个读写操作时的原子性。例如某个场景是，从某个topic消费消息，处理转换后回写到另一个topic中。  
				
				事务性保证可以使应用程序将生产数据和消费数据当作一个原子单元来处理，即使该生产或消费跨多个<Topic, Partition>。应用程序也可以在重启后，从上一个事物点恢复，也即事物恢复。  
				
				因为消息可以是跨topic和partition的，所以为实现这一效果，必须是应用程序提供一个稳定的（重启后不变）唯一的ID Transaction ID，使得PID 和 Transaction ID 一一对应起来。

		- controller集群

			controller负责partition的leader选举。以RPC的方式告知其他broker leader的变动，也负责增删Topic以及Replica的重新分配。

			- 选举算法

				第一次启动时：第一个启动的 broker 通过在 ZooKeeper 里创建一个临时节点 /controller 让自己成为 controller 控制器，其他 broker 会在这个控制器上注册一个 ZooKeeper 的 watch 对象，/controller 节点发生变化时，其他 broker 就会收到节点变更通知  
				
				宕机：跟第一次选举一样，谁抢先创建临时节点谁是controller

	- producer

		配置：  
		key.serializer：key的序列化
		value.serializer：value的序列化
		acks：指定要有多少个副本接收消息，生产者才认为消息是写入成功的
			0：producer发送出去即认为commit成功，如leader宕机，将丢失消息
			1：leader写入log即认为commit成功，如leader宕机，follower尚未复制，消息丢失
			all：ISR所有follower同步即认为commit成功
		buffer.memory：生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息
		compression.type：压缩算法类型，默认不压缩
		retries：重试次数，默认间隙100ms
		batch.size：当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。当批次被填满，批次里的所有消息会被发送出去。不过生产者井不一定都会等到批次被填满才发送，任意条数的消息都可能被发送
		client.id：任意字符串，服务器会用它来识别消息的来源，一般配置在日志里
		max.in.flight.requests.per.connection：生产者在收到服务器响应之前可以发送多少消息，值越高，就会占用越多的内存，不过也会提高吞吐量。设为1 可以保证消息是按照发送的顺序写入服务器
		request.timeout.ms：发送数据时等待服务器返回的响应时间
		metadata.fetch.timeout.ms：生产者获取元数据（比如目标分区的首领是谁）时等待服务器返回响应的时间
		max.block.ms：调用 send() 方法或使用 partitionFor() 方法获取元数据时生产者的阻塞时间当生产者的发送缓冲区已捕，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 max.block.ms 时，生产者会抛出超时异常
		max.request.size：发送的请求大小
		receive.buffer.bytes：指定了TCP Socket接收数据包缓冲区大小
		send.buffer.bytes：指定TCP SOcket发送数据包缓冲区大小

		- 三种发送方式

			- 简单发送

				ProducerRecord<String,String> record =  
				                new ProducerRecord<String, String>("CustomerCountry","West","France");  
				
				producer.send(record);

			- 基于返回值同步发送

				ProducerRecord<String,String> record =  
				                new ProducerRecord<String, String>("CustomerCountry","West","France");  
				
				try{  
				  RecordMetadata recordMetadata = producer.send(record).get();  
				}catch(Exception e){  
				  e.printStackTrace()；
				}

			- 基于回调函数异步发送

				ProducerRecord<String, String> producerRecord = new ProducerRecord<String, String>("CustomerCountry", "Huston", "America");  
				        producer.send(producerRecord,new DemoProducerCallBack());  
				
				
				class DemoProducerCallBack implements Callback {  
				
				  public void onCompletion(RecordMetadata metadata, Exception exception) {  
				    if(exception != null){  
				      exception.printStackTrace();;  
				    }  
				  }  
				}

		- 分区机制

			轮询 或者 固定key

	- consumer

		配置：  
		fetch.min.bytes：告诉Kafka，等到有足够的数据时才返回
		fetch.max.wait.ms：告诉 Kafka，在指定时间返回数据，即使数据不达到👆的参数，默认500ms
		max.partition.fetch.bytes：服务器从每个分区里返回给消费者的最大字节数，默认1MB
		session.timeout.ms：消费者没有在指定的时间内发送心跳给broker，默认3s
		heartbeat.interval.ms：poll方法向broker发送心跳频率
		auto.offset.reset：消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的该如何处理
			1）latest：在偏移量无效的情况下，消费者将从最新的记录开始读取数据
			2）earliest：在偏移量无效的情况下，消费者将从起始位置处开始读取分区的记录
		enable.auto.commit：是否自动提交偏移量，默认值是 true
		partition.assignment.strategy：决定哪些分区应该被分配给哪个消费者，range、sticky和round-robin
		client.id：客户端ID
		max.poll.records：单次调用 call() 方法能够返回的记录数量
		receive.buffer.bytes：指定了TCP Socket接收数据包缓冲区大小
		send.buffer.bytes：指定TCP SOcket发送数据包缓冲区大小

		- consumer group

			消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition
			
			消费者可以通过水平扩展的方式同时读取大量的消息。如果一个消费者失败了，其他的group成员会自动负载均衡读取之前失败的消费者读取的分区

			- 重平衡

				背景：coordinator与consumer保持心跳机制  
				
				触发条件  
				1）订阅的主题发生变化  
				2）消费者数量发生变化  
				3）分区数量发生变化  
				4）选举新的coordinator  
				
				分区策略  
				1. range：将单个topic的所有分区按照顺序排列，然后把这些分区划分成固定大小的分区段并依次分配给每个consumer  
				2.round-robin：把所有topic的所有分区顺序摆开，然后轮询式地分配给各个consumer  
				3.sticky：可以理解为重平衡后consumer消费的还是之前的partition  
				
				Generation机制  
				1.每个group进行rebalance之后，generation号都会加1，表示group进入了一个新的版本，低于该版本的请求会被拒绝  
				
				分区流程  
				1.与coordinator所在broker建立socket连接  
				2.加入组：这一步中组内所有consumer向coordinator发出JoinGroup请求，第一个发送JoinGroup的consumer自动成个leader，负责收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案  
				3.所有的消费者都加入进来并把元数据信息提交给leader后，leader做出分配方案并发送SyncGroup请求给coordinator，coordinator负责下发到消费者组  
				
				特别注意的是，重平衡期间parittion不可用

				- 五种状态

					1.EMPTY：消费者组存在但没有成员
					2.DEAD：消费者组内没有成员，且元数据被coordinator移除  
					3.PREPARDREBLANCE：消费者组开启重平衡，所有成员重新请求加入消费者组  
					4.COMPLETINGREBLANCE：所有成员加入完毕，等待分配方案  
					5.STABLE：消费者组成员稳定消费消息，重平衡结束

			- __consumer_offsets

				是kafka一个主题，记录消息偏移量，key是group.id、topic和partition的元组，value是偏移量。在第一个consumer启动的时候创建，默认分区50，副本3  
				
				提交：consumer向broker更新自己pull()得到的偏移量。因为pull可能得到很多数据，所以偏移量可能很大  
				
				重平衡时：consumer知道自己负责的partition之后，到__consumer_offsets获取该partition上次更新的偏移量，继续消费。这里就会出现两类问题

				- 两类问题

					1.消息重复消费
					consumer pull到消息，但还没消费完，也没有及时更新此次消费，于是重平衡后新的consumer读取__consumer_offsets的偏移量是旧的偏移量，也从旧的偏移量开始消费，导致消息重复消费  
					
					2.消息丢失  
					consumer pull到消息，还没消费就提前跟__consumer_offsets更新偏移量，此时发生重平衡，导致此次还没消费完的数据丢失了

				- 防止两类问题

					- 自动提交

						enable.auto.commit：开启偏移量自动提交
						auto.commit.interval.ms：interval间隙提交偏移量(默认5s)

					- 手动提交

						auto.commit.offset 设为 false

						- 同步提交

							消费者处理完消息手动调用commitSync()会一直尝试直至提交成功。如果提交失败，我们也只能把异常记录到错误日志里
							
							如果消息已经处理，提交之前发送重平衡，将导致消息重复消费  
							如果先提交，再处理消息，将导致消息丢失

						- 异步提交

							处理完消息调用commitAsync()，但有个不好的地方，假设我们发出一个请求用于提交偏移量 2000，这个时候发生了短暂的通信问题，服务器收不到请求，自然也不会作出任何响应。与此同时，我们处理了另外一批消息，并成功提交了偏移量 3000。如果 commitAsync() 重新尝试提交偏移量 2000，它有可能在偏移量 3000 之后提交成功，这个时候出现消息重复消费，所以要关闭重试  
							
							
							commitAsync() 也支持回调，在 broker 作出响应时会执行回调。回调经常被用于记录提交错误或生成度量指标。如果要用它来进行重试，则一定要注意提交的顺序  
							
							while (true) {  
							    ConsumerRecords<String, String> records = consumer.poll(100);  
							    for (ConsumerRecord<String, String> record : records) {  
							        System.out.printf("topic = %s, partition = %s,  
							        offset = %d, customer = %s, country = %s\n",  
							        record.topic(), record.partition(), record.offset(),  
							        record.key(), record.value());  
							    }  
							    consumer.commitAsync(new OffsetCommitCallback() {  
							        public void onComplete(Map<TopicPartition,  
							        OffsetAndMetadata> offsets, Exception e) {  
							            if (e != null)  
							                log.error("Commit failed for offsets {}", offsets, e);  
							        }  
							      });   
							}

						- 同步和异步混合

							try {  
							    while (true) {  
							        ConsumerRecords<String, String> records = consumer.poll(100);  
							        for (ConsumerRecord<String, String> record : records) {  
							            System.out.println("topic = %s, partition = %s, offset = %d,  
							            customer = %s, country = %s\n",  
							            record.topic(), record.partition(),  
							            record.offset(), record.key(), record.value());  
							        }  
							        consumer.commitAsync();   
							    }  
							} catch (Exception e) {  
							    log.error("Unexpected error", e);  
							} finally {  
							    try {  
							        consumer.commitSync();   
							    } finally {  
							        consumer.close();  
							    }  
							}  
							
							1.在程序正常运行过程中，我们使用 commitAsync 方法来进行提交，这样的运行速度更快，而且就算当前提交失败，下次提交成功也可以。
							2.如果直接关闭消费者，就没有所谓的“下一次提交”了，因为不会再调用poll()方法。使用 commitSync() 方法会一直重试，直到提交成功或发生无法恢复的错误。

						- 提交特定的偏移量

							private Map<TopicPartition, OffsetAndMetadata> currentOffsets =  
							    new HashMap<>();   
							int count = 0;  
							
							...  
							
							while (true) {  
							    ConsumerRecords<String, String> records = consumer.poll(100);  
							    for (ConsumerRecord<String, String> record : records)  
							    {  
							        System.out.printf("topic = %s, partition = %s, offset = %d,  
							        customer = %s, country = %s\n",  
							        record.topic(), record.partition(), record.offset(),  
							        record.key(), record.value());   
							        currentOffsets.put(new TopicPartition(record.topic(),  
							        record.partition()), new  
							        OffsetAndMetadata(record.offset()+1, "no metadata"));   
							        if (count % 1000 == 0)   
							            consumer.commitAsync(currentOffsets,null);   
							        count++;  
							    }  
							}  
							
							
							commitSync() 和 commitAsync() 将提交上次 pool() 返回的最大偏移量，若每次拉取的消息批量太大，每次又仅提交最大偏移量，则若发生 rebalance，大量消息会被重复处理，此时可以在处理批量消息中间，多次少量提交

					- 监听再均衡

						private Map<TopicPartition, OffsetAndMetadata> currentOffsets=  
						  new HashMap<>();  
						
						private class HandleRebalance implements ConsumerRebalanceListener {   
						    public void onPartitionsAssigned(Collection<TopicPartition>  
						      partitions) {   
						    }  
						
						    public void onPartitionsRevoked(Collection<TopicPartition>  
						      partitions) {  
						        System.out.println("Lost partitions in rebalance.  
						          Committing current  
						        offsets:" + currentOffsets);  
						        consumer.commitSync(currentOffsets);   
						    }  
						}  
						
						try {  
						    consumer.subscribe(topics, new HandleRebalance());   
						
						    while (true) {  
						        ConsumerRecords<String, String> records =  
						          consumer.poll(100);  
						        for (ConsumerRecord<String, String> record : records)  
						        {  
						            System.out.println("topic = %s, partition = %s, offset = %d,  
						             customer = %s, country = %s\n",  
						             record.topic(), record.partition(), record.offset(),  
						             record.key(), record.value());  
						             currentOffsets.put(new TopicPartition(record.topic(),  
						             record.partition()), new  
						             OffsetAndMetadata(record.offset()+1, "no metadata"));  
						        }  
						        consumer.commitAsync(currentOffsets, null);  
						    }  
						} catch (WakeupException e) {  
						    // 忽略异常，正在关闭消费者
						} catch (Exception e) {  
						    log.error("Unexpected error", e);  
						} finally {  
						    try {  
						        consumer.commitSync(currentOffsets);  
						    } finally {  
						        consumer.close();  
						        System.out.println("Closed consumer and we are done");  
						    }  
						}  
						
						在调用 subscribe() 方法时传进去一个 ConsumerRebalanceListener 实例就可以了。 ConsumerRebalanceListener 有两个需要实现的方法。
						
						public void onPartitionsRevoked(Collection<TopicPartition> partitions) 方法会在重平衡开始之前和消费者停止读取消息之后被调用。如果在这里提交偏移量，下一个接管分区的消费者就知道该从哪里开始读取了
						
						public void onPartitionsAssigned(Collection<TopicPartition> partitions) 方法会在重新分配分区之后和消费者开始读取消息之前被调用
						
						注意：提交的是最近处理过的偏移量，而不是批次中还在处理的最后一个偏移量

				- 两种偏移量

					Current Offset：Consumer第一次调用poll()方法后收到了20条消息，那么Current Offset就被设置为20。这样Consumer下一次调用poll()方法时，Kafka就知道应该从序号为21的消息开始读取。这样就能够保证每次Consumer poll消息时，都能够收到不重复的消息
					
					Committed Offset：consumer commit的偏移量  
					
					Current Offset是针对Consumer的poll过程的，它可以保证每次poll都返回不重复的消息；而Committed Offset是用于Consumer Rebalance过程的，它能够保证新的Consumer能够从正确的位置开始消费一个partition，从而避免重复消费。

			- 选举算法

				如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader，如果某一个时刻leader消费者由于某些原因退出了消费组，那么就会随机选举leader

		- 两种消费方式

			- The SimpleConsumer API

				1.自己控制offset，想从哪里读取就从哪里读取。  
				2.自行控制连接分区，对分区自定义进行负载均衡  
				3.对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）  
				
				低级API缺点  
				太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等

			- The high-level Consumer API

				1.高级API 写起来简单
				2.不需要自行去管理offset，系统通过zookeeper自行管理
				3.不需要管理分区，副本等情况，.系统自动管理
				4.（老版本）消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset）
				4.（新版本）__consumer_offsets  
				5.可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录6.不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）
				
				高级API缺点
				不能自行控制offset（对于某些特殊需求来说）
				不能细化控制如分区、副本、zk等

		- group Coordinator

			每个消费者组都会有一个broker负责协调（称为group coordinator），各个消费者通过发送心跳的方式向组协调者同步状态，当有消费者一定时间没有给组协调者发送心跳或者有新的消费者加入到消费者组时，就会触发消费组的重平衡操作

			- 选举

	- 总结

		1.broker会在ZK注册临时节点
		2.topic也会在ZK注册节点  
		3.consumer也会在ZK注册节点  
		4.重平衡消费者leader选举靠coordinator和消费者之间维护的心跳机制  
		5.副本leader选举靠broker和ZK的心跳机制，但是选举交给controller的broker  
		
		
		1.ISR：/brokers/topics/[topic]/partitions/[partition]/state  
		2.Controller：/controller

		- ZK的作用

			1.负责Controller节点选举
			
			broker  
			1.每个broker启动时，都会到ZooKeeper创建路径 /brokers/ids/{broker.id}，该节点是临时的  
			2.创建完节点后，Kafka 会将该 broker 的 broker.name 及端口号记录到该节点  
			
			topic  
			路径：/brokers/topics/{topic_name}  
			
			consumer（老版本）  
			/consumers/{group_id}/  
				ids：记录该消费组中当前正在消费的消费者  
				owners：记录该消费组消费的 topic 信息  
				offsets：记录每个 topic 的每个分区的 offset

		- 生产流程

			其路由机制为：  
			1.指定了 patition，则直接使用；
			2.未指定 partition 但指定 key，通过对 key 进行 hash 选出一个 partition；
			3. partition 和 key 都未指定，使用轮询选出一个 partition。
			
			写入流程：  
			1.producer 先从 ZooKeeper 的 "/brokers/.../state" 节点找到该 partition 的leader；
			2.producer 将消息发送给该 leader；
			3.leader 将消息写入本地 log；
			4.followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK；
			5.leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK；

	- 面试题

		- 所有replica宕机

			1.等待ISR中的任一个Replica“活”过来，并且选它作为Leader（一致性好，但是可用性差）
			2.选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader（一致性差，但是可用性相比第一种方式好）

		- 脑裂问题

			Controller脑裂：每当新的controller产生的时候就会在zk中生成一个全新的、数值更大的controller epoch的标识，并同步给其他的broker进行保存，这样当第二个controller发送指令时，其他的broker就会自动忽略
			
			leader脑裂：

		- 快的原因

			1.基于操作系统的pageCache：将写入的页标记为dirty，然后向外部存储flush；读数据时首先读取缓存，如果未命中，再去外部存储读取，并且将读取来的数据也加入缓存
			
			2.零拷贝：省略磁盘数据移动到用户空间步骤，节省两次cpu上下文切换 Sendfile();  
			
			3.顺序写入  
			
			4.分区写入：分摊每个机器的压力  
			
			5.批量压缩和批量发送

### 消息丢失

- RabbitMQ

	- 生产端丢失

		1）利用MQ事务机制，但会影响性能
		
		2）开启confirm模式。  
		
		3）二者差异：事务机制是同步的，会造成阻塞；confirm机制是异步的，发送消息之后可以接着发送下一个消息，然后rabbitmq会回调告知成功与否。 一般在生产者这块避免丢失，都是用confirm机制。

	- 消息队列端丢失

		设置消息持久化到磁盘：  
		①创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里面的数据。
		
		②发送消息的时候将消息的deliveryMode设置为2，这样消息就会被设为持久化方式，此时rabbitmq就会将消息持久化到磁盘上。
		
		注意：必须要同时开启这两个才可以。持久化可以跟生产的confirm机制配合起来，只有消息持久化到了磁盘之后，才会通知生产者ack，这样就算是在持久化之前rabbitmq挂了，数据丢了，生产者收不到ack回调也会进行消息重发。

	- 消费端丢失

		使用rabbitmq提供的ack机制，首先关闭rabbitmq的自动ack，然后每次在确保处理完这个消息之后，在代码里手动调用ack。这样就可以避免消息还没有处理完就ack

- RocketMQ

	- 生产端丢失

		SendStatus判断消息发送状态选择是否重发

	- broker端丢失

		1.单机模式：使用同步刷盘，收到消息立刻刷盘到磁盘
		
		2.主从模式：使用同步双写进一步保证消息同步到slave

	- 消费者端丢失

		消费者从 broker 拉取消息，然后执行相应的业务逻辑。一旦执行成功，将会返回 ConsumeConcurrentlyStatus.CONSUME_SUCCESS 状态给 Broker。如果 Broker 未收到消费确认响应或收到其他状态，消费者下次还会再次拉取到该条消息，进行重试。这样的方式有效避免了消费者消费过程发生异常，或者消息在网络传输中丢失的情况

- Kafka

	- 生产端丢失

		配置文件设置acks=all，
		在producer端设置retries=MAX，一旦写入失败，这无限重试

	- 消息队列端丢失

		①给topic设置 replication.factor参数：这个值必须大于1，表示要求每个partition必须至少有2个副本
		
		②在kafka服务端设置min.isync.replicas参数：这个值必须大于1，表示 要求一个leader至少感知到有至少一个follower在跟自己保持联系正常同步数据，这样才能保证leader挂了之后还有一个follower

	- 消费端丢失

		关闭自动提交offset，在自己处理完毕之后手动提交offse

### 消息重复消费

- RocketMQ

	1.在MQ系统，利用一张日志表来记录已经处理成功的消息的ID，如果新到的消息ID已经在日志表中，那么就不再处理这条消息
	2.在消费者，做到幂等

- kafka

	Producer端：基于kafka的幂等功能
	
	consumer端：基于__consumer_offsets

### [消息按顺序消费](https://juejin.im/post/5ce560195188253114078ae0)

- kafka

	背景：partition天然有序，因为消息是append到partition
	
	订单场景，要求订单的创建、付款、发货、收货、完成消息在同一订单下是有序发生的，即消费者在接收消息时需要保证在接收到订单发货前一定收到了订单创建和付款消息  
	
	方案：同一key分发到同一partition
	
	max.in.flight.requests.per.connection：生产者在收到服务器响应之前可以发送多少个消息。值越高，占用内存越大。为了顺序消费，可以设置成1，但牺牲吞吐量
	
	enable.idempotence=true开启幂等

- RocketMQ

	1.生产端：同一组业务消息发送到同一个队列
	
	2.消费端：一个消费者消费一个队列，如果多个消费者消费同一个队列，RocketMq支持队列锁

	- 消费者端

		Rocket采用的是分段锁，它不是锁整个Rocket而是锁里面的单个Queue，因为只要锁单个Queue就可以保证局部顺序消费了。
		
		消费者1去Queue拿 订单生成，它就锁住了整个Queue，只有它消费完成并返回成功后，这个锁才会释放。
		
		然后下一个消费者去拿到 订单支付同样锁住当前Queue，这样的一个过程来真正保证对同一个Queue能够真正意义上的顺序消费，而不仅仅是顺序取出。

		- 顺序消费（Orderly）

			消息消费的顺序同生产者为每个消息队列发送的顺序一致，所以如果正在处理全局顺序是强制性的场景，需要确保使用的主题只有一个消息队列

		- 并行消费（Concurrently）

			不再保证消息顺序，消费的最大并行数量受每个消费者客户端指定的线程池限制

	- 生产者端

		因为同一条queue消息是FIFO，将相同ID的消息投递到同个队列

### [消息服务高可用](https://juejin.im/post/5ce55e0af265da1b667bb22c)

### 防止消息积压

①先修复consumer的问题，确保其恢复消费速度，然后将现有consumer都停掉

②临时建立好原先10倍或者20倍的queue数量(新建一个topic，partition是原来的10倍)

③然后写一个临时分发消息的consumer程序，这个程序部署上去消费积压的消息，消费之后不做耗时处理，直接均匀轮询写入临时建好分10数量的queue里面

④紧接着征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的消息

⑤这种做法相当于临时将queue资源和consumer资源扩大10倍，以正常速度的10倍来消费消息

⑥等快速消费完了之后，恢复原来的部署架构，重新用原来的consumer机器来消费消息

### 减少消息延时

消费端：提升消费者消费能力，增加数量或优化代码，或者使用多线程并行消费  

消息队列端：零拷贝

